### iOS Vision Framework x WWDC 24: Discover Swift Enhancements in the Vision Framework Session

**Vision Framework Overview & iOS 18 New Swift API Exploration**

![Photo by BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash){:target="_blank"}

#### Topic

![The relationship between Vision Pro and hot dogs is like that between hot dogs and dogsâ€”nonexistent.](/assets/755509180ca8/1*ebqm2jzCK1GSrDDY0XtrUA.png)

### Vision Framework

The Vision framework is Apple's integrated machine learning image recognition framework, allowing developers to quickly and easily implement common image recognition features. It was first introduced in iOS 11.0 (2017/iPhone 8) and has been continuously improved, integrating Swift Concurrency features to enhance execution performance. With iOS 18.0, a new Swift Vision framework API is available to maximize the benefits of Swift Concurrency.

**Features of the Vision Framework**
- Includes numerous image recognition and dynamic tracking methods (31 types as of iOS 18)
- On-device processing using only the phone's chip, ensuring fast and secure recognition without relying on cloud services
- Simple and easy-to-use API
- Supported across all Apple platforms: iOS 11.0+, iPadOS 11.0+, Mac Catalyst 13.0+, macOS 10.13+, tvOS 11.0+, visionOS 1.0+
- Continuously updated since its release in 2017
- Integrated with Swift language features to enhance computational performance

> **_Six years ago, I dabbled with: [Exploring Vision â€” Automatic Face Detection and Cropping for App Profile Uploads (Swift)](../9a9aa892f9a9/)_**

> _This time, I revisited it with the [WWDC 24 Discover Swift enhancements in the Vision framework Session](https://developer.apple.com/videos/play/wwdc2024/10163/){:target="_blank"} and experimented with the new Swift features._

#### CoreML

Apple also offers another framework called [CoreML](https://developer.apple.com/documentation/coreml){:target="_blank"}, which is a machine learning framework based on on-device chips. It allows you to train your own models for object and document recognition and use them directly in your app. Interested users can explore it as well (e.g., [Real-time Article Classification](../793bf2cdda0f/), [Spam Detection](https://apps.apple.com/tw/app/%E7%86%8A%E7%8C%AB%E5%90%83%E7%9F%AD%E4%BF%A1-%E5%9E%83%E5%9C%BE%E7%9F%AD%E4%BF%A1%E8%BF%87%E6%BB%A4/id1319191852){:target="_blank"}).

#### P.S.

[**Vision**](https://developer.apple.com/documentation/vision/){:target="_blank"} **vs. [VisionKit](https://developer.apple.com/documentation/visionkit){:target="_blank"}:**

> [**_Vision_**](https://developer.apple.com/documentation/vision/){:target="_blank"} _is primarily used for image analysis tasks such as face recognition, barcode detection, and text recognition. It provides powerful APIs for processing and analyzing visual content in static images or videos._

> [**_VisionKit_**](https://developer.apple.com/documentation/visionkit){:target="_blank"} _is specifically designed for document scanning tasks. It offers a scanner view controller for scanning documents and generating high-quality PDFs or images._

The Vision framework cannot run on simulators for M1 models and requires testing on a physical device. Running it in a simulator environment results in a `Could not create Espresso context` error, and no solution was found in the [official forum discussions](https://forums.developer.apple.com/forums/thread/675806){:target="_blank"}.

> _Due to the lack of a physical iOS 18 device for testing, all execution results in this article are based on the previous (pre-iOS 18) methods. **If you encounter errors with the new methods, please leave a comment for guidance.**_

### WWDC 2024 â€” Discover Swift Enhancements in the Vision Framework

![Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"}

> _This article is a note on the WWDC 24 â€” [Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"} session, along with some personal experimental insights._

### Introduction â€” Vision Framework Features
#### Face Recognition and Contour Detection

![Face Recognition](/assets/755509180ca8/1*RNGfE_EeaQhiKAPdJeFYQw.png)

![Contour Detection](/assets/755509180ca8/1*iMdzeLm2aWjATVV6_Kvrjg.png)

#### Text Recognition in Image Content

As of iOS 18, it supports 18 languages.

![Text Recognition](/assets/755509180ca8/1*kU_OYn5w368h-ahDYU4lDw.png)

```swift
// List of supported languages
if #available(iOS 18.0, *) {
  print(RecognizeTextRequest().supportedRecognitionLanguages.map { "\($0.languageCode!)-\(($0.region?.identifier ?? $0.script?.identifier)!)" })
} else {
  print(try! VNRecognizeTextRequest().supportedRecognitionLanguages())
}

// The actual available recognition languages are based on this.
// Tested output on iOS 18:
// ["en-US", "fr-FR", "it-IT", "de-DE", "es-ES", "pt-BR", "zh-Hans", "zh-Hant", "yue-Hans", "yue-Hant", "ko-KR", "ja-JP", "ru-RU", "uk-UA", "th-TH", "vi-VT", "ar-SA", "ars-SA"]
// Swedish language mentioned in WWDC was not seen, unsure if it's not released yet or related to device region and language settings.
```

#### Dynamic Motion Capture

![Dynamic Motion Capture](/assets/755509180ca8/1*6TfyCcszdD1NdId0bdM16Q.gif)

![Gesture Capture](/assets/755509180ca8/1*8y_XXdH36uKpfP0p6BCJQA.gif)

- Enables dynamic capture of people and objects
- Gesture capture allows for air signature functionality

#### Whatâ€™s New in Vision? (iOS 18) â€” Image Scoring Feature (Quality, Memorability)
- Can calculate scores for input images to help select high-quality photos
- Scoring considers multiple dimensions, not just image quality, but also lighting, angle, subject, and **memorable elements** among others

![Image Scoring](/assets/755509180ca8/1*XwjeaHcB6arxJhIR7cFsWg.png)

![Image Scoring](/assets/755509180ca8/1*YdhZlZBlTaIZd4nLxhBtaQ.png)

![Image Scoring](/assets/755509180ca8/1*IhMDFdk6DWwTv1qIG0Gi0Q.png)

WWDC provided the above three images for explanation (under the same quality), which are:
- High-scoring image: good composition, lighting, and memorable elements
- Low-scoring image: lacks a subject, appears casually or accidentally taken
- Stock image: technically well-shot but lacks memorable elements, like images used in stock photo libraries

**iOS â‰¥ 18 New API: [CalculateImageAestheticsScoresRequest](https://developer.apple.com/documentation/vision/calculateimageaestheticsscoresrequest){:target="_blank"}**
```swift
let request = CalculateImageAestheticsScoresRequest()
let result = try await request.perform(on: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*yL3vI1ADzwlovctW5WQgJw.jpeg")!)

// Photo score
print(result.overallScore)

// Whether it is considered a stock image
print(result.isUtility)
```

#### Whatâ€™s New in Vision? (iOS 18) â€” Simultaneous Body and Gesture Pose Detection

![Body and Gesture Pose Detection](/assets/755509180ca8/1*A9320aRV-jdccgiXrmSrJw.png)

Previously, body and hand poses could only be detected separately. This update allows developers to detect body and hand poses simultaneously, combining them into a single request and result, facilitating more application development.

**iOS â‰¥ 18 New API: [DetectHumanBodyPoseRequest](https://developer.apple.com/documentation/vision/detecthumanbodyposerequest){:target="_blank"}**
```swift
var request = DetectHumanBodyPoseRequest()
// Also detect hand poses
request.detectsHands = true

guard let bodyPose = try await request.perform(on: image).first else { return }

// Body pose joints
let bodyJoints = bodyPose.allJoints()
// Left hand pose joints
let leftHandJoints = bodyPose.leftHand.allJoints()
// Right hand pose joints
let rightHandJoints = bodyPose.rightHand.allJoints()
```

### New Vision API

In this update, Apple provides a new Swift Vision API for developers, which not only supports existing features but also enhances Swift 6/Swift Concurrency features, offering more efficient and Swift-like API operations.

### Get Started with Vision

![Get Started with Vision](/assets/755509180ca8/1*mv9g5jmqrS6YScxoGYJemQ.png)

![Get Started with Vision](/assets/755509180ca8/1*iidNN7nKHoskh_tcjfuHKQ.png)

Here, the speaker reintroduces the basic usage of the Vision framework. Apple has packaged [31 types](https://developer.apple.com/documentation/vision/visionrequest){:target="_blank"} (as of iOS 18) of common image recognition requests "Request" and corresponding "Observation" objects.

1. **Request:** DetectFaceRectanglesRequest for face area recognition
   **Result:** FaceObservation
   The previous article "[Exploring Vision â€” Automatic Face Detection and Cropping for App Profile Uploads (Swift)](../9a9aa892f9a9/)" used this request.

2. **Request:** RecognizeTextRequest for text recognition
   **Result:** RecognizedTextObservation

3. **Request:** GenerateObjectnessBasedSaliencyImageRequest for main object recognition
   **Result:** SaliencyImageObservation

### All 31 Request Types:

[VisionRequest](https://developer.apple.com/documentation/vision/visionrequest){:target="_blank"}.

Here's the translated text into naturalistic English:

| Request Purpose | Observation Description |
|-----------------|-------------------------|
| CalculateImageAestheticsScoresRequest<br/>Calculate the aesthetic scores of an image. | AestheticsObservation<br/>Returns the aesthetic evaluation of an image, considering factors like composition and color. |
| ClassifyImageRequest<br/>Classify the content of an image. | ClassificationObservation<br/>Returns classification labels and confidence levels for objects or scenes in the image. |
| CoreMLRequest<br/>Analyze an image using a Core ML model. | CoreMLFeatureValueObservation<br/>Generates observations based on the output of a Core ML model. |
| DetectAnimalBodyPoseRequest<br/>Detect animal poses in an image. | RecognizedPointsObservation<br/>Returns the skeletal points and their positions for animals. |
| DetectBarcodesRequest<br/>Detect barcodes in an image. | BarcodeObservation<br/>Returns barcode data and type (e.g., QR code). |
| DetectContoursRequest<br/>Detect contours in an image. | ContoursObservation<br/>Returns the detected contour lines in the image. |
| DetectDocumentSegmentationRequest<br/>Detect and segment documents in an image. | RectangleObservation<br/>Returns the position of the rectangular boundary of the document. |
| DetectFaceCaptureQualityRequest<br/>Evaluate the quality of face capture. | FaceObservation<br/>Returns a quality assessment score for the face image. |
| DetectFaceLandmarksRequest<br/>Detect facial landmarks. | FaceObservation<br/>Returns detailed positions of facial landmarks (e.g., eyes, nose). |
| DetectFaceRectanglesRequest<br/>Detect faces in an image. | FaceObservation<br/>Returns the boundary box positions of faces. |
| DetectHorizonRequest<br/>Detect the horizon in an image. | HorizonObservation<br/>Returns the angle and position of the horizon. |
| DetectHumanBodyPose3DRequest<br/>Detect 3D human poses in an image. | RecognizedPointsObservation<br/>Returns 3D skeletal points and their spatial coordinates for humans. |
| DetectHumanBodyPoseRequest<br/>Detect human poses in an image. | RecognizedPointsObservation<br/>Returns skeletal points and their coordinates for humans. |
| DetectHumanHandPoseRequest<br/>Detect hand poses in an image. | RecognizedPointsObservation<br/>Returns skeletal points and their positions for hands. |
| DetectHumanRectanglesRequest<br/>Detect humans in an image. | HumanObservation<br/>Returns the boundary box positions of humans. |
| DetectRectanglesRequest<br/>Detect rectangles in an image. | RectangleObservation<br/>Returns the coordinates of the four vertices of rectangles. |
| DetectTextRectanglesRequest<br/>Detect text areas in an image. | TextObservation<br/>Returns the position and boundary box of text areas. |
| DetectTrajectoriesRequest<br/>Detect and analyze object motion trajectories. | TrajectoryObservation<br/>Returns trajectory points and their time sequences. |
| GenerateAttentionBasedSaliencyImageRequest<br/>Generate an attention-based saliency image. | SaliencyImageObservation<br/>Returns a saliency map of the most attractive areas in the image. |
| GenerateForegroundInstanceMaskRequest<br/>Generate a foreground instance mask image. | InstanceMaskObservation<br/>Returns the mask of foreground objects. |
| GenerateImageFeaturePrintRequest<br/>Generate an image feature print for comparison. | FeaturePrintObservation<br/>Returns feature print data of the image for similarity comparison. |
| GenerateObjectnessBasedSaliencyImageRequest<br/>Generate an objectness-based saliency image. | SaliencyImageObservation<br/>Returns a saliency map of object saliency areas. |
| GeneratePersonInstanceMaskRequest<br/>Generate a person instance mask image. | InstanceMaskObservation<br/>Returns the mask of person instances. |
| GeneratePersonSegmentationRequest<br/>Generate a person segmentation image. | SegmentationObservation<br/>Returns a binary image of person segmentation. |
| RecognizeAnimalsRequest<br/>Detect and recognize animals in an image. | RecognizedObjectObservation<br/>Returns the type of animal and its confidence level. |
| RecognizeTextRequest<br/>Detect and recognize text in an image. | RecognizedTextObservation<br/>Returns the detected text content and its area location. |
| TrackHomographicImageRegistrationRequest<br/>Track homographic image registration. | ImageAlignmentObservation<br/>Returns the homographic transformation matrix between images for alignment. |
| TrackObjectRequest<br/>Track objects in an image. | DetectedObjectObservation<br/>Returns the position and velocity information of objects in the image. |
| TrackOpticalFlowRequest<br/>Track optical flow in an image. | OpticalFlowObservation<br/>Returns the optical flow vector field to describe pixel movement. |
| TrackRectangleRequest<br/>Track rectangles in an image. | RectangleObservation<br/>Returns the position, size, and rotation angle of rectangles in the image. |
| TrackTranslationalImageRegistrationRequest<br/>Track translational image registration. | ImageAlignmentObservation<br/>Returns the translational transformation matrix between images for alignment. |

- Prefixing with VN is the old API syntax (for versions before iOS 18).

The speaker mentioned several commonly used Requests, as follows:

#### ClassifyImageRequest

Identify the input image to obtain classification labels and confidence levels.

![](/assets/755509180ca8/1*8NSQEjxGejujKLbXcILmxQ.jpeg)

![Travelogue: 2024 Second Visit to Kyushu, 9-Day Free Trip, Entering via Busanâ†’Hakata Cruise](/assets/755509180ca8/1*f1rNoOIQbE33M9F9NmoTXg.png)

[Travelogue: 2024 Second Visit to Kyushu, 9-Day Free Trip, Entering via Busanâ†’Hakata Cruise]
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = ClassifyImageRequest()
    Task {
        do {
            let observations = try await request.perform(on: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*yL3vI1ADzwlovctW5WQgJw.jpeg")!)
            observations.forEach {
                observation in
                print("\(observation.identifier): \(observation.confidence)")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old method
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNClassificationObservation] else {
            return
        }
        observations.forEach {
            observation in
            print("\(observation.identifier): \(observation.confidence)")
        }
    }

    let request = VNClassifyImageRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*3_jdrLurFuUfNdW4BJaRww.jpeg")!, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Results:**
```r
 â€¢ outdoor: 0.75392926
 â€¢ sky: 0.75392926
 â€¢ blue_sky: 0.7519531
 â€¢ machine: 0.6958008
 â€¢ cloudy: 0.26538086
 â€¢ structure: 0.15728651
 â€¢ sign: 0.14224191
 â€¢ fence: 0.118652344
 â€¢ banner: 0.0793457
 â€¢ material: 0.075975396
 â€¢ plant: 0.054406323
 â€¢ foliage: 0.05029297
 â€¢ light: 0.048126098
 â€¢ lamppost: 0.048095703
 â€¢ billboards: 0.040039062
 â€¢ art: 0.03977703
 â€¢ branch: 0.03930664
 â€¢ decoration: 0.036868922
 â€¢ flag: 0.036865234
....more
```

#### RecognizeTextRequest

Recognize text content in an image (a.k.a. image-to-text).

![[Travelogue: 2023 Tokyo 5-Day Free Trip](../9da2c51fa4f2/)](/assets/755509180ca8/1*XL40lLT774PfO60rCIfnxA.jpeg)

[Travelogue: 2023 Tokyo 5-Day Free Trip](../9da2c51fa4f2/)
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    var request = RecognizeTextRequest()
    request.recognitionLevel = .accurate
    request.recognitionLanguages = [.init(identifier: "ja-JP"), .init(identifier: "en-US")] // Specify language code, e.g., Traditional Chinese
    Task {
        do {
            let observations = try await request.perform(on: URL(string: "https://zhgchg.li/assets/9da2c51fa4f2/1*fBbNbDepYioQ-3-0XUkF6Q.jpeg")!)
            observations.forEach {
                observation in
                let topCandidate = observation.topCandidates(1).first
                print(topCandidate?.string ?? "No text recognized")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old method
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedTextObservation] else {
            return
        }
        observations.forEach {
            observation in
            let topCandidate = observation.topCandidates(1).first
            print(topCandidate?.string ?? "No text recognized")
        }
    }

    let request = VNRecognizeTextRequest(completionHandler: completionHandler)
    request.recognitionLevel = .accurate
    request.recognitionLanguages = ["ja-JP", "en-US"] // Specify language code, e.g., Traditional Chinese
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: URL(string: "https://zhgchg.li/assets/9da2c51fa4f2/1*fBbNbDepYioQ-3-0XUkF6Q.jpeg")!, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Results:**
```makefile
LE LABO Aoyama Store
TEL:03-6419-7167
*Thank you for your purchase*
No: 21347
Date: 2023/06/10 14.14.57
Clerk:
1690370
Register: 008A 1
Product Name
Tax-Inclusive Price Quantity Total
Kayak 10 EDP FB 15ML
J1P7010000S
16,800
16,800
Another 13 EDP FB 15ML
J1PJ010000S
10,700
10,700
Lip Balm 15ML
JOWC010000S
2,000
1
Total Amount
(Tax Included)
CARD
2,000
3 items purchased
29,500
0
29,500
29,500
```

#### DetectBarcodesRequest

Detect barcode and QR code data in an image.

![](/assets/755509180ca8/1*Z72y9rIwIKQCmnnuwsq0uQ.png)

![Recommended by Locals: Goose Brand Cooling Balm](/assets/755509180ca8/1*s3V1UQRIqto-iG1e30PK7Q.jpeg)

Recommended by Locals: Goose Brand Cooling Balm
```swift
let filePath = Bundle.main.path(forResource: "IMG_6777", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = DetectBarcodesRequest()
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            observations.forEach {
                observation in
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old method
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        observations.forEach {
            observation in
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let request = VNDetectBarcodesRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

This translation provides a naturalistic English version of the original text, maintaining the technical details and context.

**Analysis Results:**

```makefile
Payload: 8859126000911
Symbology: VNBarcodeSymbologyEAN13
Payload: https://lin.ee/hGynbVM
Symbology: VNBarcodeSymbologyQR
Payload: http://www.hongthaipanich.com/
Symbology: VNBarcodeSymbologyQR
Payload: https://www.facebook.com/qr?id=100063856061714
Symbology: VNBarcodeSymbologyQR
```

#### RecognizeAnimalsRequest

Identify animals in the image and their confidence levels.

![Image](path/to/image.png)

![Meme Source](https://www.redbubble.com/i/canvas-print/Funny-AI-Woman-yelling-at-a-cat-meme-design-Machine-learning-by-omolog/43039298.5Y5V7)

```swift
let filePath = Bundle.main.path(forResource: "IMG_5026", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // Using new Swift features API
    let request = RecognizeAnimalsRequest()
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            observations.forEach { observation in
                let labels = observation.labels
                labels.forEach { label in
                    print("Detected animal: \(label.identifier) with confidence: \(label.confidence)")
                }
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Older method
    let completionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedObjectObservation] else {
            return
        }
        observations.forEach { observation in
            let labels = observation.labels
            labels.forEach { label in
                print("Detected animal: \(label.identifier) with confidence: \(label.confidence)")
            }
        }
    }

    let request = VNRecognizeAnimalsRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

Analysis Result:
```csharp
Detected animal: Cat with confidence: 0.77245045
```

#### Others:
- Detect human figures in images: DetectHumanRectanglesRequest
- Detect poses of humans and animals (3D or 2D): DetectAnimalBodyPoseRequest, DetectHumanBodyPose3DRequest, DetectHumanBodyPoseRequest, DetectHumanHandPoseRequest
- Detect and track object motion trajectories (in videos, animations across frames): DetectTrajectoriesRequest, TrackObjectRequest, TrackRectangleRequest

#### **iOS â‰¥ 18 Update Highlight:**
```rust
VN*Request -> *Request (e.g. VNDetectBarcodesRequest -> DetectBarcodesRequest)
VN*Observation -> *Observation (e.g. VNRecognizedObjectObservation -> RecognizedObjectObservation)
VNRequestCompletionHandler -> async/await
VNImageRequestHandler.perform([VN*Request]) -> *Request.perform()
```

### WWDC Example

The official WWDC video uses a supermarket product scanner as an example.

#### Most products have a Barcode for scanning

![Image](path/to/image.png)

We can obtain the location of the Barcode from `observation.boundingBox`, but unlike the common UIView coordinate system, the `BoundingBox` starts from the bottom left, with values ranging from 0 to 1.

```swift
let filePath = Bundle.main.path(forResource: "IMG_6785", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // Using new Swift features API
    var request = DetectBarcodesRequest()
    request.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            if let observation = observations.first {
                DispatchQueue.main.async {
                    self.infoLabel.text = observation.payloadString
                    // Mark color Layer
                    let colorLayer = CALayer()
                    // iOS >=18 new coordinate conversion API toImageCoordinates
                    // Untested, actual implementation may require calculating displacement for ContentMode = AspectFit:
                    colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                    colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                    self.baseImageView.layer.addSublayer(colorLayer)
                }
                print("BoundingBox: \(observation.boundingBox.cgRect)")
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Older method
    let completionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        if let observation = observations.first {
            DispatchQueue.main.async {
                self.infoLabel.text = observation.payloadStringValue
                // Mark color Layer
                let colorLayer = CALayer()
                colorLayer.frame = self.convertBoundingBox(observation.boundingBox, to: self.baseImageView)
                colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                self.baseImageView.layer.addSublayer(colorLayer)
            }
            print("BoundingBox: \(observation.boundingBox)")
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let request = VNDetectBarcodesRequest(completionHandler: completionHandler)
    request.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

**iOS â‰¥ 18 Update Highlight:**
```less
// iOS >=18 new coordinate conversion API toImageCoordinates
observation.boundingBox.toImageCoordinates(CGSize, origin: .upperLeft)
// https://developer.apple.com/documentation/vision/normalizedpoint/toimagecoordinates(from:imagesize:origin:)
```

**Helper:**
```swift
// Gen by ChatGPT 4o
// Because the image in ImageView is set to ContentMode = AspectFit
// Additional calculation for blank displacement due to Fit is needed
func convertBoundingBox(_ boundingBox: CGRect, to view: UIImageView) -> CGRect {
    guard let image = view.image else {
        return .zero
    }

    let imageSize = image.size
    let viewSize = view.bounds.size
    let imageRatio = imageSize.width / imageSize.height
    let viewRatio = viewSize.width / viewSize.height
    var scaleFactor: CGFloat
    var offsetX: CGFloat = 0
    var offsetY: CGFloat = 0
    if imageRatio > viewRatio {
        // Image fits in width direction
        scaleFactor = viewSize.width / imageSize.width
        offsetY = (viewSize.height - imageSize.height * scaleFactor) / 2
    } else {
        // Image fits in height direction
        scaleFactor = viewSize.height / imageSize.height
        offsetX = (viewSize.width - imageSize.width * scaleFactor) / 2
    }

    let x = boundingBox.minX * imageSize.width * scaleFactor + offsetX
    let y = (1 - boundingBox.maxY) * imageSize.height * scaleFactor + offsetY
    let width = boundingBox.width * imageSize.width * scaleFactor
    let height = boundingBox.height * imageSize.height * scaleFactor
    return CGRect(x: x, y: y, width: width, height: height)
}
```

**Output Result**
```makefile
BoundingBox: (0.5295758928571429, 0.21408638121589782, 0.0943080357142857, 0.21254415360708087)
Payload: 4710018183805
Symbology: VNBarcodeSymbologyEAN13
```

#### Some products do not have a Barcode, such as loose fruits with only product labels

![Image](path/to/image.png)

Therefore, our scanner also needs to support scanning plain text labels.

```swift
let filePath = Bundle.main.path(forResource: "apple", ofType: "jpg")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // Using new Swift features API
    var barcodesRequest = DetectBarcodesRequest()
    barcodesRequest.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    var textRequest = RecognizeTextRequest()
    textRequest.recognitionLanguages = [.init(identifier: "zh-Hnat"), .init(identifier: "en-US")]
    Task {
        do {
            let handler = ImageRequestHandler(fileURL)
            // Parameter pack syntax and we must wait for all requests to finish before we can use their results.
            // let (barcodesObservation, textObservation, ...) = try await handler.perform(barcodesRequest, textRequest, ...)
            let (barcodesObservation, textObservation) = try await handler.perform(barcodesRequest, textRequest)
            if let observation = barcodesObservation.first {
                DispatchQueue.main.async {
                    self.infoLabel.text = observation.payloadString
                    // Mark color Layer
                    let colorLayer = CALayer()
                    // iOS >=18 new coordinate conversion API toImageCoordinates
                    // Untested, actual implementation may require calculating displacement for ContentMode = AspectFit:
                    colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                    colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                    self.baseImageView.layer.addSublayer(colorLayer)
                }
                print("BoundingBox: \(observation.boundingBox.cgRect)")
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
            textObservation.forEach { observation in
                let topCandidate = observation.topCandidates(1).first
                print(topCandidate?.string ?? "No text recognized")
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Older method
    let barcodesCompletionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        if let observation = observations.first {
            DispatchQueue.main.async {
                self.infoLabel.text = observation.payloadStringValue
                // Mark color Layer
                let colorLayer = CALayer()
                colorLayer.frame = self.convertBoundingBox(observation.boundingBox, to: self.baseImageView)
                colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                self.baseImageView.layer.addSublayer(colorLayer)
            }
            print("BoundingBox: \(observation.boundingBox)")
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let textCompletionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedTextObservation] else {
            return
        }
        observations.forEach { observation in
            let topCandidate = observation.topCandidates(1).first
            print(topCandidate?.string ?? "No text recognized")
        }
    }

    let barcodesRequest = VNDetectBarcodesRequest(completionHandler: barcodesCompletionHandler)
    barcodesRequest.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    let textRequest = VNRecognizeTextRequest(completionHandler: textCompletionHandler)
    textRequest.recognitionLevel = .accurate
    textRequest.recognitionLanguages = ["en-US"]
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([barcodesRequest, textRequest])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Output Result:**
```
94128s
ORGANIC
Pink LadyÂ®
Produce of USh
```

**iOS â‰¥ 18 Update Highlight:**
```swift
let handler = ImageRequestHandler(fileURL)
// Parameter pack syntax and we must wait for all requests to finish before we can use their results.
// let (barcodesObservation, textObservation, ...) = try await handler.perform(barcodesRequest, textRequest, ...)
let (barcodesObservation, textObservation) = try await handler.perform(barcodesRequest, textRequest)
```

#### iOS â‰¥ 18 [performAll()](https://developer.apple.com/documentation/vision/imagerequesthandler/performall(_:)?changes=latest_minor) Method

![Image](path/to/image.png)

The previous `perform(barcodesRequest, textRequest)` method for handling Barcode and text scanning requires waiting for both requests to complete before proceeding; starting with iOS 18, a new `performAll()` method is available, changing the response method to streaming, allowing for immediate response when a request result is received, such as responding immediately when a Barcode is scanned.

```swift
if #available(iOS 18.0, *) {
    // Using new Swift features API
    var barcodesRequest = DetectBarcodesRequest()
    barcodesRequest.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    var textRequest = RecognizeTextRequest()
    textRequest.recognitionLanguages = [.init(identifier: "zh-Hnat"), .init(identifier: "en-US")]
    Task {
        let handler = ImageRequestHandler(fileURL)
        let observation = handler.performAll([barcodesRequest, textRequest] as [any VisionRequest])
        for try await result in observation {
            switch result {
                case .detectBarcodes(_, let barcodesObservation):
                if let observation = barcodesObservation.first {
                    DispatchQueue.main.async {
                        self.infoLabel.text = observation.payloadString
                        // Mark color Layer
                        let colorLayer = CALayer()
                        // iOS >=18 new coordinate conversion API toImageCoordinates
                        // Untested, actual implementation may require calculating displacement for ContentMode = AspectFit:
                        colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                        colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                        self.baseImageView.layer.addSublayer(colorLayer)
                    }
                    print("BoundingBox: \(observation.boundingBox.cgRect)")
                    print("Payload: \(observation.payloadString ?? "No payload")")
                    print("Symbology: \(observation.symbology)")
                }
                case .recognizeText(_, let textObservation):
                textObservation.forEach { observation in
                    let topCandidate = observation.topCandidates(1).first
                    print(topCandidate?.string ?? "No text recognized")
                }
                default:
                print("Unrecognized result: \(result)")
            }
        }
    }
}
```

### Optimize with Swift Concurrency

![Image](path/to/image.png)

![Image](path/to/image.png)

Assume we have a list of images on a wall, and each image needs to be automatically cropped to highlight the main object. In this case, we can utilize Swift Concurrency to improve loading efficiency.

#### **Original Implementation**
```swift
func generateThumbnail(url: URL) async throws -> UIImage {
  let request = GenerateAttentionBasedSaliencyImageRequest()
  let saliencyObservation = try await request.perform(on: url)
  return cropImage(url, to: saliencyObservation.salientObjects)
}
    
func generateAllThumbnails() async throws {
  for image in images {
    image.thumbnail = try await generateThumbnail(url: image.url)
  }
}
```

This approach processes one image at a time, resulting in slow efficiency and performance.

#### **Optimization (1) â€” TaskGroup Concurrency**
```swift
func generateAllThumbnails() async throws {
  try await withThrowingDiscardingTaskGroup { taskGroup in
    for image in images {
      image.thumbnail = try await generateThumbnail(url: image.url)
    }
  }
}
```

Here, each task is added to a TaskGroup for concurrent execution.

> **_Issue: Image recognition and cropping operations are very memory-intensive. Uncontrolled addition of concurrent tasks may lead to user interface lag or out-of-memory (OOM) crashes._**

#### Optimization (2) â€” TaskGroup Concurrency + Limiting Concurrency
```swift
func generateAllThumbnails() async throws {
    try await withThrowingDiscardingTaskGroup { taskGroup in
        // Limit the number of concurrent tasks to a maximum of 5
        let maxImageTasks = min(5, images.count)
        // Initially fill 5 tasks
        for index in 0..<maxImageTasks {
            taskGroup.addTask {
                image[index].thumbnail = try await generateThumbnail(url: image[index].url)
            }
        }
        var nextIndex = maxImageTasks
        for try await _ in taskGroup {
            // When a task in the taskGroup completes...
            // Check if the index has reached the end
            if nextIndex < images.count {
                let image = images[nextIndex]
                // Continue adding tasks one by one (maintaining a maximum of 5)
                taskGroup.addTask {
                    image.thumbnail = try await generateThumbnail(url: image.url)
                }
                nextIndex += 1
            }
        }
    }
}
```

### Update an Existing Vision App

1. Vision will remove CPU and GPU support for certain requests on devices with a neural engine. On these devices, the neural engine is the best choice for performance. You can check using the `supportedComputeDevices()` API.
2. Remove all VN prefixes: `VNXXRequest`, `VNXXXObservation` -> `Request`, `Observation`.
3. Replace the original VNRequestCompletionHandler with async/await.
4. Use `*Request.perform()` directly instead of the original `VNImageRequestHandler.perform([VN*Request])`.

### Wrap-up
- APIs are newly designed for Swift language features.
- New functionalities and methods are Swift-only, available on iOS 18 and above.
- New features include image scoring and body + hand motion tracking.

### Thanks!

### KKday Recruitment

ðŸ‘‰ðŸ‘‰ðŸ‘‰ This study group sharing is from KKday App Team's weekly technical sharing activity. **The team is currently actively recruiting a [Senior iOS Engineer](https://kkday.bamboohr.com/careers/25?source=aWQ9Mjk%3D). Interested friends are welcome to apply.** ðŸ‘ˆðŸ‘ˆðŸ‘ˆ

#### Reference Materials
#### [Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/)

The Vision Framework API has been redesigned to leverage modern Swift features like concurrency, making it easier and faster to integrate a wide array of Vision algorithms into your app. Weâ€™ll tour the updated API and share sample code, along with best practices, to help you get the benefits of this framework with less coding effort. Weâ€™ll also demonstrate two new features: image aesthetics and holistic body pose.

### Chapters
- 0:00 â€” [Introduction](https://developer.apple.com/videos/play/wwdc2024/10163/?time=0)
- 1:07 â€” [New Vision API](https://developer.apple.com/videos/play/wwdc2024/10163/?time=67)
- 1:47 â€” [Get started with Vision](https://developer.apple.com/videos/play/wwdc2024/10163/?time=107)
- 8:59 â€” [Optimize with Swift Concurrency](https://developer.apple.com/videos/play/wwdc2024/10163/?time=539)
- 11:05 â€” [Update an existing Vision app](https://developer.apple.com/videos/play/wwdc2024/10163/?time=665)
- 13:46 â€” [Whatâ€™s new in Vision?](https://developer.apple.com/videos/play/wwdc2024/10163/?time=826)

#### [Vision framework Apple Developer Documentation](https://developer.apple.com/documentation/vision/)

If you have any questions or feedback, feel free to [contact me](https://www.zhgchg.li/contact).

_[Post](https://medium.com/kkdaytech/ios-vision-framework-x-wwdc-24-discover-swift-enhancements-in-the-vision-framework-session-755509180ca8) converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown)._