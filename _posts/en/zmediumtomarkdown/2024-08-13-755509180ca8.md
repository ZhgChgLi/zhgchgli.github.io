---
title: "iOS Vision Framework at WWDC 24: Discover Swift Enhancements in the Vision Framework Session"
author: "ZhgChgLi"
date: 2024-08-13T08:10:37.015+0000
last_modified_at: 2024-08-14T12:07:49.774+0000
categories: ["KKday Tech Blog"]
tags: ["ios-app-development", "vision-framework", "apple-intelligence", "ai", "machine-learning"]
description: "A review of the Vision framework features & hands-on with the new Swift API in iOS 18"
image:
  path: /assets/755509180ca8/1*NqN-_MAE4tt11n6MnUQWxQ.jpeg
render_with_liquid: false
---

### iOS Vision Framework at WWDC 24: Discover Swift Enhancements in the Vision Framework Session

A review of the Vision framework features & hands-on with the new Swift API in iOS 18.

![Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash){:target="_blank"}](/assets/755509180ca8/1*NqN-_MAE4tt11n6MnUQWxQ.jpeg)

Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash){:target="_blank"}

#### Topic

![The relationship between Vision Pro and hot dogs is as unrelated as it gets.](/assets/755509180ca8/1*ebqm2jzCK1GSrDDY0XtrUA.png)

The relationship between Vision Pro and hot dogs is as unrelated as it gets.

### Vision Framework

The Vision framework is Apple's integrated image recognition framework that leverages machine learning, allowing developers to easily and quickly implement common image recognition features. Launched with iOS 11.0 (in 2017 with the iPhone 8), the framework has undergone continuous iterations and optimizations, enhancing its integration with Swift Concurrency to improve execution performance. Starting with iOS 18.0, it introduces a brand new Swift Vision framework API to maximize the benefits of Swift Concurrency.

**Key Features of the Vision Framework:**
- Built-in methods for various image recognition and dynamic tracking tasks (31 methods available as of iOS 18)
- On-device processing using the phone's chip, ensuring quick and secure recognition without relying on cloud services
- Simple and user-friendly API
- Supported across all Apple platforms: iOS 11.0+, iPadOS 11.0+, Mac Catalyst 13.0+, macOS 10.13+, tvOS 11.0+, visionOS 1.0+
- Continuously updated since its launch in 2017
- Enhanced computational performance through integration with Swift language features

> **_Six years ago, I briefly explored: [An Introduction to Vision — Automatic Face Cropping for App Profile Pictures (Swift)](../9a9aa892f9a9/)_**

> _This time, I revisited the framework in conjunction with the [WWDC 24 Discover Swift Enhancements in the Vision Framework Session](https://developer.apple.com/videos/play/wwdc2024/10163/){:target="_blank"} and experimented with the new Swift features._

#### CoreML

Apple also offers another framework called [CoreML](https://developer.apple.com/documentation/coreml){:target="_blank"}, which is a machine learning framework based on on-device processing. It allows you to train your own models for recognizing objects and documents, which can then be integrated directly into your app. Interested developers can explore this as well (e.g., [Real-time Article Classification](../793bf2cdda0f/) and real-time [Spam Detection](https://apps.apple.com/tw/app/%E7%86%8A%E7%8C%AB%E5%90%83%E7%9F%AD%E4%BF%A1-%E5%9E%83%E5%9C%BE%E7%9F%AD%E4%BF%A1%E8%BF%87%E6%BB%A4/id1319191852){:target="_blank"}).

#### P.S.

[**Vision**](https://developer.apple.com/documentation/vision/){:target="_blank"} **vs. [VisionKit](https://developer.apple.com/documentation/visionkit){:target="_blank"}:**

> [**_Vision_**](https://developer.apple.com/documentation/vision/){:target="_blank"} _is primarily used for image analysis tasks such as face recognition, barcode detection, and text recognition. It provides powerful APIs for processing and analyzing visual content in static images or videos._

> [**_VisionKit_**](https://developer.apple.com/documentation/visionkit){:target="_blank"} _is specifically designed for tasks related to document scanning. It provides a scanner view controller that can be used to scan documents and generate high-quality PDFs or images._

The Vision framework cannot run on M1 models in the simulator; it must be tested on a physical device. Running it in a simulator environment will throw a `Could not create Espresso context` error. I checked the [official forum discussions but couldn't find a solution](https://forums.developer.apple.com/forums/thread/675806){:target="_blank"}.

> _Since I do not have a physical iOS 18 device for testing, all execution results in this article are based on older methods (pre-iOS 18). **If any errors arise with the new methods, please feel free to leave a comment.**_

### WWDC 2024 — Discover Swift Enhancements in the Vision Framework

![[Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"}](/assets/755509180ca8/1*8N5GtY1uqxP-4iAAAticOA.png)

[Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"} 

> _This article shares notes from the WWDC 24 session on [Discover Swift Enhancements in the Vision Framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"} along with some personal experimental insights._

### Introduction — Vision Framework Features

#### Face Recognition and Contour Detection

![](/assets/755509180ca8/1*RNGfE_EeaQhiKAPdJeFYQw.png)

![](/assets/755509180ca8/1*iMdzeLm2aWjATVV6_Kvrjg.png)

#### Text Recognition in Image Content

As of iOS 18, it supports 18 languages.

![](/assets/755509180ca8/1*kU_OYn5w368h-ahDYU4lDw.png)

```swift
// List of supported languages
if #available(iOS 18.0, *) {
  print(RecognizeTextRequest().supportedRecognitionLanguages.map { "\($0.languageCode!)-\(($0.region?.identifier ?? $0.script?.identifier)!)" })
} else {
  print(try! VNRecognizeTextRequest().supportedRecognitionLanguages())
}

// The actual available recognition languages are as follows:
// The output from iOS 18 includes:
// ["en-US", "fr-FR", "it-IT", "de-DE", "es-ES", "pt-BR", "zh-Hans", "zh-Hant", "yue-Hans", "yue-Hant", "ko-KR", "ja-JP", "ru-RU", "uk-UA", "th-TH", "vi-VT", "ar-SA", "ars-SA"]
// I did not see the Swedish language mentioned at WWDC; it's unclear if it hasn't been released yet or if it's related to device region or language settings.
```

#### Dynamic Motion Capture

![](/assets/755509180ca8/1*6TfyCcszdD1NdId0bdM16Q.gif)

![](/assets/755509180ca8/1*8y_XXdH36uKpfP0p6BCJQA.gif)

- Enables dynamic capture of people and objects
- Gesture recognition for air signature functionality

#### What’s New in Vision? (iOS 18) — Image Scoring Feature (Quality, Memorable Points)
- Can calculate scores for input images to help filter out high-quality photos
- Scoring criteria include multiple dimensions, not just image quality, but also lighting, angle, subject matter, and **whether it evokes memorable points**.

![](/assets/755509180ca8/1*XwjeaHcB6arxJhIR7cFsWg.png)

![](/assets/755509180ca8/1*YdhZlZBlTaIZd4nLxhBtaQ.png)

![](/assets/755509180ca8/1*IhMDFdk6DWwTv1qIG0Gi0Q.png)

During WWDC, the above three images were used for illustration (under the same quality):
- High-scoring image: good composition, lighting, and memorable points
- Low-scoring image: lacks a subject, appears to be a casual or accidental shot
- Utility image: technically well-taken but lacks memorable points, like stock images.

**iOS ≥ 18 New API: [CalculateImageAestheticsScoresRequest](https://developer.apple.com/documentation/vision/calculateimageaestheticsscoresrequest){:target="_blank"}**
```swift
let request = CalculateImageAestheticsScoresRequest()
let result = try await request.perform(on: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*yL3vI1ADzwlovctW5WQgJw.jpeg")!)

// Photo score
print(result.overallScore)

// Whether it is classified as a utility image
print(result.isUtility)
```

#### What’s New in Vision? (iOS 18) — Simultaneous Detection of Body and Gesture Poses

![](/assets/755509180ca8/1*A9320aRV-jdccgiXrmSrJw.png)

Previously, body poses and hand poses could only be detected individually. This update allows developers to detect both body and hand poses simultaneously, combining them into a single request and result, facilitating the development of more application features.

**iOS ≥ 18 New API: [DetectHumanBodyPoseRequest](https://developer.apple.com/documentation/vision/detecthumanbodyposerequest){:target="_blank"}**
```swift
var request = DetectHumanBodyPoseRequest()
// Simultaneously detect hand poses
request.detectsHands = true

guard let bodyPose = try await request.perform(on: image).first else { return }

// Body Pose Joints
let bodyJoints = bodyPose.allJoints()
// Left Hand Pose Joints
let leftHandJoints = bodyPose.leftHand.allJoints()
// Right Hand Pose Joints
let rightHandJoints = bodyPose.rightHand.allJoints()
```

### New Vision API

In this update, Apple has provided new Swift Vision API wrappers for developers. In addition to supporting the original features, these APIs are designed to enhance Swift 6 / Swift Concurrency capabilities, offering better performance and a more Swift-like coding experience.

### Get Started with Vision

![](/assets/755509180ca8/1*mv9g5jmqrS6YScxoGYJemQ.png)

![](/assets/755509180ca8/1*iidNN7nKHoskh_tcjfuHKQ.png)

The speaker reintroduced the basic usage of the Vision framework, which Apple has packaged into [31 common image recognition requests](https://developer.apple.com/documentation/vision/visionrequest){:target="_blank"} (as of iOS 18) along with their corresponding return "Observation" objects.
1. **Request:** DetectFaceRectanglesRequest for face area recognition
   **Result:** FaceObservation
   The previous article "[An Introduction to Vision — Automatic Face Cropping for App Profile Pictures (Swift)](../9a9aa892f9a9/)" used this request.
2. **Request:** RecognizeTextRequest for text recognition
   **Result:** RecognizedTextObservation
3. **Request:** GenerateObjectnessBasedSaliencyImageRequest for subject object recognition
   **Result:** SaliencyImageObservation

### All 31 Request Types:

[VisionRequest](https://developer.apple.com/documentation/vision/visionrequest){:target="_blank"}.


Here’s the translated text in naturalistic English:

| Request Purpose                                 | Observation Description                                                  |
|-------------------------------------------------|--------------------------------------------------------------------------|
| CalculateImageAestheticsScoresRequest           | AestheticsObservation: Returns the aesthetic scores of the image, considering factors like composition and color.                           |
| ClassifyImageRequest                             | ClassificationObservation: Returns classification labels and confidence levels for objects or scenes in the image.                           |
| CoreMLRequest                                   | CoreMLFeatureValueObservation: Generates observations based on the output of a Core ML model analyzing the image.                            |
| DetectAnimalBodyPoseRequest                     | RecognizedPointsObservation: Returns the skeletal points and their positions for animals detected in the image.                                         |
| DetectBarcodesRequest                           | BarcodeObservation: Returns barcode data and types (e.g., QR code).                                 |
| DetectContoursRequest                           | ContoursObservation: Returns the detected contour lines in the image.                                         |
| DetectDocumentSegmentationRequest               | RectangleObservation: Returns the rectangular boundaries of detected documents in the image.                                         |
| DetectFaceCaptureQualityRequest                 | FaceObservation: Returns a quality assessment score for the facial image.                                       |
| DetectFaceLandmarksRequest                      | FaceObservation: Returns detailed positions of facial landmarks (e.g., eyes, nose, etc.).                       |
| DetectFaceRectanglesRequest                      | FaceObservation: Returns the boundary box positions of detected faces in the image.                                             |
| DetectHorizonRequest                            | HorizonObservation: Returns the angle and position of the horizon in the image.                                           |
| DetectHumanBodyPose3DRequest                    | RecognizedPointsObservation: Returns the 3D skeletal points and their spatial coordinates for human figures in the image.                                    |
| DetectHumanBodyPoseRequest                       | RecognizedPointsObservation: Returns the skeletal points and their coordinates for human figures in the image.                                           |
| DetectHumanHandPoseRequest                       | RecognizedPointsObservation: Returns the skeletal points and their positions for hands detected in the image.                                           |
| DetectHumanRectanglesRequest                     | HumanObservation: Returns the boundary box positions of detected human figures in the image.                                             |
| DetectRectanglesRequest                          | RectangleObservation: Returns the coordinates of the four vertices of detected rectangles in the image.                                           |
| DetectTextRectanglesRequest                      | TextObservation: Returns the positions and boundary boxes of text areas detected in the image.                                       |
| DetectTrajectoriesRequest                        | TrajectoryObservation: Returns the points of motion trajectories and their time series.                                       |
| GenerateAttentionBasedSaliencyImageRequest      | SaliencyImageObservation: Returns a saliency map highlighting the most attractive areas in the image.                             |
| GenerateForegroundInstanceMaskRequest           | InstanceMaskObservation: Returns a mask for foreground objects in the image.                                               |
| GenerateImageFeaturePrintRequest                | FeaturePrintObservation: Returns feature fingerprint data of the image for similarity comparison.                           |
| GenerateObjectnessBasedSaliencyImageRequest     | SaliencyImageObservation: Returns a saliency map for areas of object significance in the image.                                   |
| GeneratePersonInstanceMaskRequest               | InstanceMaskObservation: Returns a mask for detected persons in the image.                                               |
| GeneratePersonSegmentationRequest                | SegmentationObservation: Returns a binary image for person segmentation.                                             |
| RecognizeAnimalsRequest                          | RecognizedObjectObservation: Returns the types of animals detected and their confidence levels.                                           |
| RecognizeTextRequest                            | RecognizedTextObservation: Returns the detected text content and its area location in the image.                                 |
| TrackHomographicImageRegistrationRequest        | ImageAlignmentObservation: Returns the homographic transformation matrix between images for alignment purposes.                           |
| TrackObjectRequest                               | DetectedObjectObservation: Returns the position and speed information of objects tracked in the image.                                 |
| TrackOpticalFlowRequest                          | OpticalFlowObservation: Returns the optical flow vector field describing pixel movement in the image.                             |
| TrackRectangleRequest                            | RectangleObservation: Returns the position, size, and rotation angle of rectangles tracked in the image.                           |
| TrackTranslationalImageRegistrationRequest      | ImageAlignmentObservation: Returns the translational transformation matrix between images for alignment purposes.                           |

- Prefixing with VN indicates the old API format (for versions prior to iOS 18).

The speaker mentioned several commonly used requests, as follows:

#### ClassifyImageRequest

Identifies the input image and provides classification labels along with confidence levels.

![Image](https://example.com/image1.jpeg)

![Travel Blog](/assets/755509180ca8/1*f1rNoOIQbE33M9F9NmoTXg.png)

[Travel Blog] 2024 Second Visit to Kyushu: 9-Day Free Trip via Busan → Hakata Cruise Entry
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = ClassifyImageRequest()
    Task {
        do {
            let observations = try await request.perform(on: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*yL3vI1ADzwlovctW5WQgJw.jpeg")!)
            observations.forEach { observation in
                print("\(observation.identifier): \(observation.confidence)")
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old format
    let completionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNClassificationObservation] else {
            return
        }
        observations.forEach { observation in
            print("\(observation.identifier): \(observation.confidence)")
        }
    }

    let request = VNClassifyImageRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*3_jdrLurFuUfNdW4BJaRww.jpeg")!, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Results:**
```r
 • outdoor: 0.75392926
 • sky: 0.75392926
 • blue_sky: 0.7519531
 • machine: 0.6958008
 • cloudy: 0.26538086
 • structure: 0.15728651
 • sign: 0.14224191
 • fence: 0.118652344
 • banner: 0.0793457
 • material: 0.075975396
 • plant: 0.054406323
 • foliage: 0.05029297
 • light: 0.048126098
 • lamppost: 0.048095703
 • billboards: 0.040039062
 • art: 0.03977703
 • branch: 0.03930664
 • decoration: 0.036868922
 • flag: 0.036865234
.... and more
```

#### RecognizeTextRequest

Recognizes the text content in images (also known as image-to-text).

![Travel Blog](https://example.com/image2.jpeg)

[Travel Blog] 2023 Tokyo: 5-Day Free Trip
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    var request = RecognizeTextRequest()
    request.recognitionLevel = .accurate
    request.recognitionLanguages = [.init(identifier: "ja-JP"), .init(identifier: "en-US")] // Specify language codes, e.g., Traditional Chinese
    Task {
        do {
            let observations = try await request.perform(on: URL(string: "https://zhgchg.li/assets/9da2c51fa4f2/1*fBbNbDepYioQ-3-0XUkF6Q.jpeg")!)
            observations.forEach { observation in
                let topCandidate = observation.topCandidates(1).first
                print(topCandidate?.string ?? "No text recognized")
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old format
    let completionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedTextObservation] else {
            return
        }
        observations.forEach { observation in
            let topCandidate = observation.topCandidates(1).first
            print(topCandidate?.string ?? "No text recognized")
        }
    }

    let request = VNRecognizeTextRequest(completionHandler: completionHandler)
    request.recognitionLevel = .accurate
    request.recognitionLanguages = ["ja-JP", "en-US"] // Specify language codes, e.g., Traditional Chinese
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: URL(string: "https://zhgchg.li/assets/9da2c51fa4f2/1*fBbNbDepYioQ-3-0XUkF6Q.jpeg")!, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Results:**
```makefile
LE LABO 青山店
TEL:03-6419-7167
＊Thank you for your purchase*
No: 21347
Date: 2023/06/10 14:14:57
Staff:
1690370
Register: 008A 1
Product Name
Tax Included Price Quantity Total Price
Kaiak 10 EDP FB 15ML
J1P7010000S
16,800
16,800
Another 13 EDP FB 15ML
J1PJ010000S
10,700
10,700
Lip Balm 15ML
JOWC010000S
2,000
1
Total Amount
(Tax Included)
CARD
2,000
3 items purchased
29,500
0
29,500
29,500
```

#### DetectBarcodesRequest

Detects barcodes and QR code data in images.

![Image](https://example.com/image3.png)

![Thai Local Recommendation: Goose Brand Cooling Balm](https://example.com/image4.jpeg)

Thai locals recommend Goose Brand Cooling Balm.
```swift
let filePath = Bundle.main.path(forResource: "IMG_6777", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = DetectBarcodesRequest()
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            observations.forEach { observation in
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old format
    let completionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        observations.forEach { observation in
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let request = VNDetectBarcodesRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

This translation maintains the original meaning while making it more readable and natural in English.

**Analysis Results:**
```makefile
Payload: 8859126000911
Symbology: VNBarcodeSymbologyEAN13
Payload: https://lin.ee/hGynbVM
Symbology: VNBarcodeSymbologyQR
Payload: http://www.hongthaipanich.com/
Symbology: VNBarcodeSymbologyQR
Payload: https://www.facebook.com/qr?id=100063856061714
Symbology: VNBarcodeSymbologyQR
```
#### RecognizeAnimalsRequest

Identify animals in the image along with their confidence levels.

![](/assets/755509180ca8/1*5zF3gA3WB1Q0-_cgt6mTCw.png)

![[meme Source](https://www.redbubble.com/i/canvas-print/Funny-AI-Woman-yelling-at-a-cat-meme-design-Machine-learning-by-omolog/43039298.5Y5V7){:target="_blank"}](/assets/755509180ca8/1*KZ7mdE8fobP-_oj7tJf_Ww.jpeg)

[meme Source](https://www.redbubble.com/i/canvas-print/Funny-AI-Woman-yelling-at-a-cat-meme-design-Machine-learning-by-omolog/43039298.5Y5V7){:target="_blank"}

```swift
let filePath = Bundle.main.path(forResource: "IMG_5026", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = RecognizeAnimalsRequest()
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            observations.forEach { observation in
                let labels = observation.labels
                labels.forEach { label in
                    print("Detected animal: \(label.identifier) with confidence: \(label.confidence)")
                }
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Older method
    let completionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedObjectObservation] else {
            return
        }
        observations.forEach { observation in
            let labels = observation.labels
            labels.forEach { label in
                print("Detected animal: \(label.identifier) with confidence: \(label.confidence)")
            }
        }
    }

    let request = VNRecognizeAnimalsRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Results:**
```csharp
Detected animal: Cat with confidence: 0.77245045
```
#### Additional Information:
- Detect humans in images: DetectHumanRectanglesRequest
- Detect poses of humans and animals (both 3D and 2D): DetectAnimalBodyPoseRequest, DetectHumanBodyPose3DRequest, DetectHumanBodyPoseRequest, DetectHumanHandPoseRequest
- Detect and track the motion trajectory of objects (in videos or animations): DetectTrajectoriesRequest, TrackObjectRequest, TrackRectangleRequest

#### **iOS ≥ 18 Update Highlights:**
```rust
VN*Request -> *Request (e.g. VNDetectBarcodesRequest -> DetectBarcodesRequest)
VN*Observation -> *Observation (e.g. VNRecognizedObjectObservation -> RecognizedObjectObservation)
VNRequestCompletionHandler -> async/await
VNImageRequestHandler.perform([VN*Request]) -> *Request.perform()
```
### WWDC Example

The official WWDC video uses a supermarket product scanner as an example.
#### Most products have barcodes that can be scanned.

![](/assets/755509180ca8/1*YT_Uf8eEi36Iv7zcOrmP4A.png)

![](/assets/755509180ca8/1*J9uIwRKubLoJoC7i096AdQ.png)

![](/assets/755509180ca8/1*gKg-NfHYqy7uBqe5hxzBSw.png)

We can obtain the location of the barcode from `observation.boundingBox`, but unlike the common UIView coordinate system, the `BoundingBox`'s relative position starts from the bottom left, with values ranging from 0 to 1.
```swift
let filePath = Bundle.main.path(forResource: "IMG_6785", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    var request = DetectBarcodesRequest()
    request.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            if let observation = observations.first {
                DispatchQueue.main.async {
                    self.infoLabel.text = observation.payloadString
                    // Color marking layer
                    let colorLayer = CALayer()
                    // iOS ≥18 new coordinate conversion API toImageCoordinates
                    // Not tested; actual calculations may require adjustments for ContentMode = AspectFit:
                    colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                    colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                    self.baseImageView.layer.addSublayer(colorLayer)
                }
                print("BoundingBox: \(observation.boundingBox.cgRect)")
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Older method
    let completionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        if let observation = observations.first {
            DispatchQueue.main.async {
                self.infoLabel.text = observation.payloadStringValue
                // Color marking layer
                let colorLayer = CALayer()
                colorLayer.frame = self.convertBoundingBox(observation.boundingBox, to: self.baseImageView)
                colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                self.baseImageView.layer.addSublayer(colorLayer)
            }
            print("BoundingBox: \(observation.boundingBox)")
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let request = VNDetectBarcodesRequest(completionHandler: completionHandler)
    request.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

**iOS ≥ 18 Update Highlights:**
```less
// iOS ≥18 new coordinate conversion API toImageCoordinates
observation.boundingBox.toImageCoordinates(CGSize, origin: .upperLeft)
// https://developer.apple.com/documentation/vision/normalizedpoint/toimagecoordinates(from:imagesize:origin:)
```

**Helper:**
```swift
// Generated by ChatGPT 4o
// Since the photo in the ImageView is set with ContentMode = AspectFit
// We need to calculate the vertical offset caused by the fit
func convertBoundingBox(_ boundingBox: CGRect, to view: UIImageView) -> CGRect {
    guard let image = view.image else {
        return .zero
    }

    let imageSize = image.size
    let viewSize = view.bounds.size
    let imageRatio = imageSize.width / imageSize.height
    let viewRatio = viewSize.width / viewSize.height
    var scaleFactor: CGFloat
    var offsetX: CGFloat = 0
    var offsetY: CGFloat = 0
    if imageRatio > viewRatio {
        // Image fits in width
        scaleFactor = viewSize.width / imageSize.width
        offsetY = (viewSize.height - imageSize.height * scaleFactor) / 2
    } else {
        // Image fits in height
        scaleFactor = viewSize.height / imageSize.height
        offsetX = (viewSize.width - imageSize.width * scaleFactor) / 2
    }

    let x = boundingBox.minX * imageSize.width * scaleFactor + offsetX
    let y = (1 - boundingBox.maxY) * imageSize.height * scaleFactor + offsetY
    let width = boundingBox.width * imageSize.width * scaleFactor
    let height = boundingBox.height * imageSize.height * scaleFactor
    return CGRect(x: x, y: y, width: width, height: height)
}
```

**Output Results:**
```makefile
BoundingBox: (0.5295758928571429, 0.21408638121589782, 0.0943080357142857, 0.21254415360708087)
Payload: 4710018183805
Symbology: VNBarcodeSymbologyEAN13
```
#### Some products do not have barcodes, such as bulk fruits that only have product labels.

![](/assets/755509180ca8/1*jeZhLtg9j11kgOAvKZmevg.jpeg)

![](/assets/755509180ca8/1*YNokMMUewMA2kzjoGmMJPw.png)

Therefore, our scanner also needs to support scanning plain text labels.
```swift
let filePath = Bundle.main.path(forResource: "apple", ofType: "jpg")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    var barcodesRequest = DetectBarcodesRequest()
    barcodesRequest.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    var textRequest = RecognizeTextRequest()
    textRequest.recognitionLanguages = [.init(identifier: "zh-Hnat"), .init(identifier: "en-US")]
    Task {
        do {
            let handler = ImageRequestHandler(fileURL)
            // Parameter pack syntax; we must wait for all requests to finish before using their results.
            // let (barcodesObservation, textObservation, ...) = try await handler.perform(barcodesRequest, textRequest, ...)
            let (barcodesObservation, textObservation) = try await handler.perform(barcodesRequest, textRequest)
            if let observation = barcodesObservation.first {
                DispatchQueue.main.async {
                    self.infoLabel.text = observation.payloadString
                    // Color marking layer
                    let colorLayer = CALayer()
                    // iOS ≥18 new coordinate conversion API toImageCoordinates
                    // Not tested; actual calculations may require adjustments for ContentMode = AspectFit:
                    colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                    colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                    self.baseImageView.layer.addSublayer(colorLayer)
                }
                print("BoundingBox: \(observation.boundingBox.cgRect)")
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
            textObservation.forEach { observation in
                let topCandidate = observation.topCandidates(1).first
                print(topCandidate?.string ?? "No text recognized")
            }
        } catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Older method
    let barcodesCompletionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        if let observation = observations.first {
            DispatchQueue.main.async {
                self.infoLabel.text = observation.payloadStringValue
                // Color marking layer
                let colorLayer = CALayer()
                colorLayer.frame = self.convertBoundingBox(observation.boundingBox, to: self.baseImageView)
                colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                self.baseImageView.layer.addSublayer(colorLayer)
            }
            print("BoundingBox: \(observation.boundingBox)")
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let textCompletionHandler: VNRequestCompletionHandler = { request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedTextObservation] else {
            return
        }
        observations.forEach { observation in
            let topCandidate = observation.topCandidates(1).first
            print(topCandidate?.string ?? "No text recognized")
        }
    }

    let barcodesRequest = VNDetectBarcodesRequest(completionHandler: barcodesCompletionHandler)
    barcodesRequest.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    let textRequest = VNRecognizeTextRequest(completionHandler: textCompletionHandler)
    textRequest.recognitionLevel = .accurate
    textRequest.recognitionLanguages = ["en-US"]
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([barcodesRequest, textRequest])
        } catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Output Results:**
```
94128s
ORGANIC
Pink Lady®
Produce of USh
```

**iOS ≥ 18 Update Highlights:**
```swift
let handler = ImageRequestHandler(fileURL)
// Parameter pack syntax; we must wait for all requests to finish before using their results.
// let (barcodesObservation, textObservation, ...) = try await handler.perform(barcodesRequest, textRequest, ...)
let (barcodesObservation, textObservation) = try await handler.perform(barcodesRequest, textRequest)
```
#### iOS ≥ 18 [performAll()](https://developer.apple.com/documentation/vision/imagerequesthandler/performall(_:)?changes=latest_minor){:target="_blank"} Method

![](/assets/755509180ca8/1*z0364eYD4F4On194EgQ1kQ.png)

The previous `perform(barcodesRequest, textRequest)` method requires waiting for both requests to complete before proceeding; starting with iOS 18, a new `performAll()` method is provided, allowing for streaming responses. This means that as soon as one of the request results is received, corresponding actions can be taken, such as responding immediately upon scanning a barcode.
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    var barcodesRequest = DetectBarcodesRequest()
    barcodesRequest.symbologies = [.ean13] // Specify to scan only EAN13 Barcode for better performance
    var textRequest = RecognizeTextRequest()
    textRequest.recognitionLanguages = [.init(identifier: "zh-Hnat"), .init(identifier: "en-US")]
    Task {
        let handler = ImageRequestHandler(fileURL)
        let observation = handler.performAll([barcodesRequest, textRequest] as [any VisionRequest])
        for try await result in observation {
            switch result {
                case .detectBarcodes(_, let barcodesObservation):
                    if let observation = barcodesObservation.first {
                        DispatchQueue.main.async {
                            self.infoLabel.text = observation.payloadString
                            // Color marking layer
                            let colorLayer = CALayer()
                            // iOS ≥18 new coordinate conversion API toImageCoordinates
                            // Not tested; actual calculations may require adjustments for ContentMode = AspectFit:
                            colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                            colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                            self.baseImageView.layer.addSublayer(colorLayer)
                        }
                        print("BoundingBox: \(observation.boundingBox.cgRect)")
                        print("Payload: \(observation.payloadString ?? "No payload")")
                        print("Symbology: \(observation.symbology)")
                    }
                case .recognizeText(_, let textObservation):
                    textObservation.forEach { observation in
                        let topCandidate = observation.topCandidates(1).first
                        print(topCandidate?.string ?? "No text recognized")
                    }
                default:
                    print("Unrecognized result: \(result)")
            }
        }
    }
}
```
### Optimize with Swift Concurrency

![](/assets/755509180ca8/1*LgxxMOVS6is3n6EqPWqA6Q.png)

![](/assets/755509180ca8/1*80CFJpkb-gjy3bJs4jAC2A.png)

Assuming we have a list of image thumbnails, where each image needs to be automatically cropped to focus on the main subject, we can effectively utilize Swift Concurrency to enhance loading efficiency.

#### **Original Implementation**
```swift
func generateThumbnail(url: URL) async throws -> UIImage {
  let request = GenerateAttentionBasedSaliencyImageRequest()
  let saliencyObservation = try await request.perform(on: url)
  return cropImage(url, to: saliencyObservation.salientObjects)
}
    
func generateAllThumbnails() async throws {
  for image in images {
    image.thumbnail = try await generateThumbnail(url: image.url)
  }
}
```

This approach processes one image at a time, resulting in slow efficiency and performance.

#### **Optimization (1) — TaskGroup Concurrency**
```swift
func generateAllThumbnails() async throws {
  try await withThrowingDiscardingTaskGroup { taskGroup in
    for image in images {
      image.thumbnail = try await generateThumbnail(url: image.url)
    }
  }
}
```

By adding each task to a TaskGroup, we can execute them concurrently.

> **_Note: Image recognition and cropping operations are very memory-intensive. If we recklessly increase the number of concurrent tasks, it may lead to user lag or out-of-memory (OOM) crashes._**

#### **Optimization (2) — TaskGroup Concurrency + Limiting Concurrent Tasks**
```swift
func generateAllThumbnails() async throws {
    try await withThrowingDiscardingTaskGroup { taskGroup in
        // Limit the maximum number of concurrent tasks to 5
        let maxImageTasks = min(5, images.count)
        // Initially fill 5 tasks
        for index in 0..<maxImageTasks {
            taskGroup.addTask {
                images[index].thumbnail = try await generateThumbnail(url: images[index].url)
            }
        }
        var nextIndex = maxImageTasks
        for try await _ in taskGroup {
            // When a task in the task group completes...
            // Check if we have reached the end of the index
            if nextIndex < images.count {
                let image = images[nextIndex]
                // Continue adding tasks (keeping the count at a maximum of 5)
                taskGroup.addTask {
                    image.thumbnail = try await generateThumbnail(url: image.url)
                }
                nextIndex += 1
            }
        }
    }
}
```

### Update an Existing Vision App

1. Vision will remove CPU and GPU support for certain requests on devices equipped with a Neural Engine. On these devices, the Neural Engine is the best option for performance. You can check this using the `supportedComputeDevices()` API.
2. Remove all `VN` prefixes: `VNXXRequest`, `VNXXXObservation` → `Request`, `Observation`.
3. Use async/await instead of the original `VNRequestCompletionHandler`.
4. Directly use `*Request.perform()` instead of the previous `VNImageRequestHandler.perform([VN*Request])`.

### Wrap-Up
- APIs are redesigned to leverage new Swift language features.
- New functionalities and methods are Swift-only and available for iOS ≥ 18.
- New features include image scoring and holistic body and hand motion tracking.

### Thank You!

### KKday Recruitment Announcement

👉👉👉 This presentation is based on the weekly technical sharing sessions within the KKday App Team. **The team is currently actively recruiting [Senior iOS Engineers](https://kkday.bamboohr.com/careers/25?source=aWQ9Mjk%3D){:target="_blank"}; interested candidates are welcome to apply!** 👈👈👈

#### References
#### [Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/){:target="_blank"}

The Vision Framework API has been redesigned to leverage modern Swift features like concurrency, making it easier and faster to integrate a wide array of Vision algorithms into your app. We’ll tour the updated API and share sample code, along with best practices, to help you get the benefits of this framework with less coding effort. We’ll also demonstrate two new features: image aesthetics and holistic body pose.

### Chapters
- 0:00 — [Introduction](https://developer.apple.com/videos/play/wwdc2024/10163/?time=0){:target="_blank"}
- 1:07 — [New Vision API](https://developer.apple.com/videos/play/wwdc2024/10163/?time=67){:target="_blank"}
- 1:47 — [Get started with Vision](https://developer.apple.com/videos/play/wwdc2024/10163/?time=107){:target="_blank"}
- 8:59 — [Optimize with Swift Concurrency](https://developer.apple.com/videos/play/wwdc2024/10163/?time=539){:target="_blank"}
- 11:05 — [Update an existing Vision app](https://developer.apple.com/videos/play/wwdc2024/10163/?time=665){:target="_blank"}
- 13:46 — [What’s new in Vision?](https://developer.apple.com/videos/play/wwdc2024/10163/?time=826){:target="_blank"}

#### [Vision framework Apple Developer Documentation](https://developer.apple.com/documentation/vision/){:target="_blank"}

If you have any questions or feedback, feel free to [contact me](https://www.zhgchg.li/contact){:target="_blank"}.

_[Post](https://medium.com/kkdaytech/ios-vision-framework-x-wwdc-24-discover-swift-enhancements-in-the-vision-framework-session-755509180ca8){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._