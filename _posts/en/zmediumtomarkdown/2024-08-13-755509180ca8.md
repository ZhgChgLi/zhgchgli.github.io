---
title: "iOS Vision framework x WWDC 24 Discover Swift enhancements in the Vision framework Session"
author: "ZhgChgLi"
date: 2024-08-13T08:10:37.015+0000
last_modified_at: 2024-08-14T12:07:49.774+0000
categories: ["KKday Tech Blog"]
tags: ["ios-app-development","vision-framework","apple-intelligence","ai","machine-learning"]
description: "Overview of Vision framework features & hands-on with new Swift API in iOS 18"
image:
  path: /assets/755509180ca8/1*NqN-_MAE4tt11n6MnUQWxQ.jpeg
render_with_liquid: false
---

### iOS Vision framework x WWDC 24 Discover Swift enhancements in the Vision framework Session

Overview of Vision framework features & hands-on with new Swift API in iOS 18


![Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash){:target="_blank"}](/assets/755509180ca8/1*NqN-_MAE4tt11n6MnUQWxQ.jpeg)

Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash){:target="_blank"}
#### Topic


![The relationship with Vision Pro is as unrelated as a hot dog is to a dog.](/assets/755509180ca8/1*ebqm2jzCK1GSrDDY0XtrUA.png)

The relationship with Vision Pro is as unrelated as a hot dog is to a dog.
### Vision framework

The Vision framework is Apple's image recognition framework that integrates machine learning, allowing developers to easily and quickly implement common image recognition features. The Vision framework was launched back in iOS 11.0+ (2017/iPhone 8) and has been continuously iterated and optimized, enhancing its integration with Swift Concurrency to improve execution performance. Starting from iOS 18.0, it offers a brand new Swift Vision framework API to maximize the benefits of Swift Concurrency.

**Features of the Vision framework**
- Built-in numerous image recognition and dynamic tracking methods (31 types as of iOS 18)
- On-Device processing using only the phone's chip, with recognition processes not relying on cloud services, making it fast and secure
- Simple and easy-to-use API
- Supported across all Apple platforms: iOS 11.0+, iPadOS 11.0+, Mac Catalyst 13.0+, macOS 10.13+, tvOS 11.0+, visionOS 1.0+
- Released for several years (2017-present) and continuously updated
- Integrates Swift language features to enhance computational performance



> **_Six years ago, I played around with: [An Introduction to Vision — Automatic Face Cropping for App Profile Pictures (Swift)](../9a9aa892f9a9/)_** 





> _This time, I revisited it in conjunction with the [WWDC 24 Discover Swift enhancements in the Vision framework Session](https://developer.apple.com/videos/play/wwdc2024/10163/){:target="_blank"} and combined it with new Swift features for another round of exploration._ 




#### CoreML

Apple also has another framework called [CoreML](https://developer.apple.com/documentation/coreml){:target="_blank"}, which is also a machine learning framework based on On-Device chips; however, it allows you to train your own object and document models and integrate them directly into your app. Interested friends can give it a try. (e.g. [Real-time Article Classification](../793bf2cdda0f/), real-time [Spam Detection](https://apps.apple.com/tw/app/%E7%86%8A%E7%8C%AB%E5%90%83%E7%9F%AD%E4%BF%A1-%E5%9E%83%E5%9C%BE%E7%9F%AD%E4%BF%A1%E8%BF%87%E6%BB%A4/id1319191852){:target="_blank"} …)
#### p.s.

[**Vision**](https://developer.apple.com/documentation/vision/){:target="_blank"} **v\.s\. [VisionKit](https://developer.apple.com/documentation/visionkit){:target="_blank"} ：**


> [**_Vision_**](https://developer.apple.com/documentation/vision/){:target="_blank"} _is primarily used for image analysis tasks such as face recognition, barcode detection, text recognition, and more. It provides powerful APIs to handle and analyze visual content in static images or videos._ 





> [**_VisionKit_**](https://developer.apple.com/documentation/visionkit){:target="_blank"} _is specifically designed for tasks related to document scanning. It provides a scanner view controller that can be used to scan documents and generate high-quality PDFs or images._ 





The Vision framework cannot run on the simulator for M1 models and can only be tested on physical devices; running in the simulator environment will throw a `Could not create Espresso context` Error. See [the official forum discussion, no answers found](https://forums.developer.apple.com/forums/thread/675806){:target="_blank"}.


> _Since I do not have a physical iOS 18 device for testing, all execution results in this article are based on the results from older versions (prior to iOS 18); **please leave a comment if there are errors in the new syntax**._ 




### WWDC 2024 — Discover Swift enhancements in the Vision framework


![[Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"}](/assets/755509180ca8/1*8N5GtY1uqxP-4iAAAticOA.png)

[Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"}


> _This article shares notes from the WWDC 24 — [Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/?time=45){:target="_blank"} session, along with some personal experimental insights._ 




### Introduction — Vision framework Features
#### Face Recognition, Contour Detection


![](/assets/755509180ca8/1*RNGfE_EeaQhiKAPdJeFYQw.png)



![](/assets/755509180ca8/1*iMdzeLm2aWjATVV6_Kvrjg.png)

#### Text Recognition in Image Content

As of iOS 18, supports 18 languages.


![](/assets/755509180ca8/1*kU_OYn5w368h-ahDYU4lDw.png)

```swift
// List of supported languages
if #available(iOS 18.0, *) {
  print(RecognizeTextRequest().supportedRecognitionLanguages.map { "\($0.languageCode!)-\(($0.region?.identifier ?? $0.script?.identifier)!)" })
} else {
  print(try! VNRecognizeTextRequest().supportedRecognitionLanguages())
}

// The actual available recognition languages are based on this.
// Testing on iOS 18 outputs the following results:
// ["en-US", "fr-FR", "it-IT", "de-DE", "es-ES", "pt-BR", "zh-Hans", "zh-Hant", "yue-Hans", "yue-Hant", "ko-KR", "ja-JP", "ru-RU", "uk-UA", "th-TH", "vi-VT", "ar-SA", "ars-SA"]
// Did not see the Swedish language mentioned at WWDC, unsure if it has not been released yet or if it is related to device region and language settings.
```
#### Dynamic Motion Capture

![](/assets/755509180ca8/1*6TfyCcszdD1NdId0bdM16Q.gif)

![](/assets/755509180ca8/1*8y_XXdH36uKpfP0p6BCJQA.gif)

- Dynamic capture of people and objects can be achieved.
- Gesture capture enables air signature functionality.

#### What’s new in Vision? \(iOS 18\)— Image Scoring Feature \(Quality, Memory Points\)
- Scores can be calculated for input images, making it easier to filter out high-quality photos.
- The scoring method includes multiple dimensions, not just image quality, but also lighting, angle, subject matter, **and whether it evokes any memorable points** … etc.

![](/assets/755509180ca8/1*XwjeaHcB6arxJhIR7cFsWg.png)

![](/assets/755509180ca8/1*YdhZlZBlTaIZd4nLxhBtaQ.png)

![](/assets/755509180ca8/1*IhMDFdk6DWwTv1qIG0Gi0Q.png)

The three images above were presented at WWDC for explanation (under the same image quality), which are:
- High-scoring image: composition, lighting, has memorable points.
- Low-scoring image: lacks a subject, appears to be taken casually or accidentally.
- Material image: technically well-taken but lacks memorable points, like images used for stock photo libraries.

**iOS ≥ 18 New API: [CalculateImageAestheticsScoresRequest](https://developer.apple.com/documentation/vision/calculateimageaestheticsscoresrequest){:target="_blank"}**
```swift
let request = CalculateImageAestheticsScoresRequest()
let result = try await request.perform(on: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*yL3vI1ADzwlovctW5WQgJw.jpeg")!)

// Photo score
print(result.overallScore)

// Whether it is classified as a material image
print(result.isUtility)
```
#### What’s new in Vision? \(iOS 18\) — Simultaneous Detection of Body + Gesture Poses

![](/assets/755509180ca8/1*A9320aRV-jdccgiXrmSrJw.png)

Previously, only individual detection of human body poses and hand poses was possible. This update allows developers to simultaneously detect body poses + hand poses, combining them into a single request and result, facilitating the development of more application features.

**iOS ≥ 18 New API: [DetectHumanBodyPoseRequest](https://developer.apple.com/documentation/vision/detecthumanbodyposerequest){:target="_blank"}**
```swift
var request = DetectHumanBodyPoseRequest()
// Also detect hand poses
request.detectsHands = true

guard let bodyPose = try await request.perform(on: image).first else { return }

// Body pose joints
let bodyJoints = bodyPose.allJoints()
// Left hand pose joints
let leftHandJoints = bodyPose.leftHand.allJoints()
// Right hand pose joints
let rightHandJoints = bodyPose.rightHand.allJoints()
```
### New Vision API

In this update, Apple has provided new Swift Vision API wrappers for developers to use. In addition to basic support for existing functionalities, it mainly enhances the features of Swift 6 / Swift Concurrency, offering more efficient and Swift-like API operations.

### Get started with Vision

![](/assets/755509180ca8/1*mv9g5jmqrS6YScxoGYJemQ.png)

![](/assets/755509180ca8/1*iidNN7nKHoskh_tcjfuHKQ.png)

Here, the speaker reintroduced the basic usage of the Vision framework. Apple has packaged [31 types](https://developer.apple.com/documentation/vision/visionrequest){:target="_blank"} (as of iOS 18) of common image recognition requests "Request" and the corresponding returned "Observation" objects.
1. **Request:** DetectFaceRectanglesRequest for face area recognition
   **Result:** FaceObservation
   The previous article "[Exploring Vision — Automatic Face Cropping for App Avatar Upload \(Swift\)](../9a9aa892f9a9/)" used this pair of requests.
2. **Request:** RecognizeTextRequest for text recognition
   **Result:** RecognizedTextObservation
3. **Request:** GenerateObjectnessBasedSaliencyImageRequest for subject object recognition
   **Result:** SaliencyImageObservation

### All 31 Types of Requests:

[VisionRequest](https://developer.apple.com/documentation/vision/visionrequest){:target="_blank"} .

| Purpose of Request                                 | Description of Observation                                                  |
|---------------------------------------------------|--------------------------------------------------------------------------|
| CalculateImageAestheticsScoresRequest<br/>Calculate the aesthetic scores of an image.                                 | AestheticsObservation<br/>Returns the aesthetic ratings of the image, such as composition, color, and other factors.                           |
| ClassifyImageRequest<br/>Classify the content of an image.                                      | ClassificationObservation<br/>Returns classification labels and confidence levels for objects or scenes in the image.                           |
| CoreMLRequest<br/>Analyze the image using a Core ML model.                          | CoreMLFeatureValueObservation<br/>Generates observations based on the output of the Core ML model.                            |
| DetectAnimalBodyPoseRequest<br/>Detect the pose of animals in the image.                               | RecognizedPointsObservation<br/>Returns the skeletal points of the animal and their locations.                                         |
| DetectBarcodesRequest<br/>Detect barcodes in the image.                                   | BarcodeObservation<br/>Returns barcode data and types (e.g., QR code).                                 |
| DetectContoursRequest<br/>Detect contours in the image.                                   | ContoursObservation<br/>Returns the detected contour lines in the image.                                         |
| DetectDocumentSegmentationRequest<br/>Detect and segment documents in the image.                             | RectangleObservation<br/>Returns the rectangular boundary positions of the document.                                         |
| DetectFaceCaptureQualityRequest<br/>Evaluate the quality of face capture.                                   | FaceObservation<br/>Returns the quality assessment score of the face image.                                       |
| DetectFaceLandmarksRequest<br/>Detect facial landmarks.                                     | FaceObservation<br/>Returns the detailed locations of facial landmarks (e.g., eyes, nose, etc.).                       |
| DetectFaceRectanglesRequest<br/>Detect faces in the image.                                   | FaceObservation<br/>Returns the boundary box positions of the faces.                                             |
| DetectHorizonRequest<br/>Detect the horizon in the image.                                 | HorizonObservation<br/>Returns the angle and position of the horizon.                                           |
| DetectHumanBodyPose3DRequest<br/>Detect 3D human body poses in the image.                           | RecognizedPointsObservation<br/>Returns the 3D skeletal points of the human body and their spatial coordinates.                                    |
| DetectHumanBodyPoseRequest<br/>Detect human body poses in the image.                               | RecognizedPointsObservation<br/>Returns the skeletal points of the human body and their coordinates.                                           |
| DetectHumanHandPoseRequest<br/>Detect hand poses in the image.                               | RecognizedPointsObservation<br/>Returns the skeletal points of the hand and their locations.                                           |
| DetectHumanRectanglesRequest<br/>Detect humans in the image.                                   | HumanObservation<br/>Returns the boundary box positions of the human body.                                             |
| DetectRectanglesRequest<br/>Detect rectangles in the image.                                   | RectangleObservation<br/>Returns the coordinates of the four vertices of the rectangle.                                           |
| DetectTextRectanglesRequest<br/>Detect text areas in the image.                               | TextObservation<br/>Returns the positions and boundary boxes of the text areas.                                       |
| DetectTrajectoriesRequest<br/>Detect and analyze the motion trajectories of objects.                             | TrajectoryObservation<br/>Returns the motion trajectory points and their time series.                                       |
| GenerateAttentionBasedSaliencyImageRequest<br/>Generate an attention-based saliency image.                         | SaliencyImageObservation<br/>Returns a saliency map of the most attractive areas in the image.                             |
| GenerateForegroundInstanceMaskRequest<br/>Generate a foreground instance mask image.                               | InstanceMaskObservation<br/>Returns the mask of the foreground object.                                               |
| GenerateImageFeaturePrintRequest<br/>Generate an image feature fingerprint for comparison.                         | FeaturePrintObservation<br/>Returns the feature fingerprint data of the image for similarity comparison.                           |
| GenerateObjectnessBasedSaliencyImageRequest<br/>Generate an objectness-based saliency image.                           | SaliencyImageObservation<br/>Returns a saliency map of the object salient areas.                                   |
| GeneratePersonInstanceMaskRequest<br/>Generate a person instance mask image.                               | InstanceMaskObservation<br/>Returns the mask of the person instance.                                               |
| GeneratePersonSegmentationRequest<br/>Generate a person segmentation image.                                   | SegmentationObservation<br/>Returns a binary image of the person segmentation.                                             |
| RecognizeAnimalsRequest<br/>Detect and recognize animals in the image.                             | RecognizedObjectObservation<br/>Returns the type of animal and its confidence level.                                           |
| RecognizeTextRequest<br/>Detect and recognize text in the image.                             | RecognizedTextObservation<br/>Returns the detected text content and its area location.                                 |
| TrackHomographicImageRegistrationRequest<br/>Track the homographic image registration.                             | ImageAlignmentObservation<br/>Returns the homography transformation matrix between images for image registration.                           |
| TrackObjectRequest<br/>Track objects in the image.                                   | DetectedObjectObservation<br/>Returns the position and velocity information of the object in the image.                                 |
| TrackOpticalFlowRequest<br/>Track optical flow in the image.                                   | OpticalFlowObservation<br/>Returns the optical flow vector field to describe pixel movement.                             |
| TrackRectangleRequest<br/>Track rectangles in the image.                                   | RectangleObservation<br/>Returns the position, size, and rotation angle of the rectangle in the image.                           |
| TrackTranslationalImageRegistrationRequest<br/>Track the translational image registration.                             | ImageAlignmentObservation<br/>Returns the translational transformation matrix between images for image registration.                           |

- Adding VN in front refers to the old API syntax (for iOS versions prior to 18).

The speaker mentioned several commonly used requests, as follows.
#### ClassifyImageRequest

Recognizes the input image and obtains label classifications and confidence levels.

![](/assets/755509180ca8/1*8NSQEjxGejujKLbXcILmxQ.jpeg)

![\[Travelogue\] 2024 Second Visit to Kyushu 9-Day Free Trip, Entering via Busan → Hakata Cruise](/assets/755509180ca8/1*f1rNoOIQbE33M9F9NmoTXg.png)

\[Travelogue\] 2024 Second Visit to Kyushu 9-Day Free Trip, Entering via Busan → Hakata Cruise
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = ClassifyImageRequest()
    Task {
        do {
            let observations = try await request.perform(on: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*yL3vI1ADzwlovctW5WQgJw.jpeg")!)
            observations.forEach {
                observation in
                print("\(observation.identifier): \(observation.confidence)")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old syntax
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNClassificationObservation] else {
            return
        }
        observations.forEach {
            observation in
            print("\(observation.identifier): \(observation.confidence)")
        }
    }

    let request = VNClassifyImageRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: URL(string: "https://zhgchg.li/assets/cb65fd5ab770/1*3_jdrLurFuUfNdW4BJaRww.jpeg")!, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Results:**
```r
 • outdoor: 0.75392926
 • sky: 0.75392926
 • blue_sky: 0.7519531
 • machine: 0.6958008
 • cloudy: 0.26538086
 • structure: 0.15728651
 • sign: 0.14224191
 • fence: 0.118652344
 • banner: 0.0793457
 • material: 0.075975396
 • plant: 0.054406323
 • foliage: 0.05029297
 • light: 0.048126098
 • lamppost: 0.048095703
 • billboards: 0.040039062
 • art: 0.03977703
 • branch: 0.03930664
 • decoration: 0.036868922
 • flag: 0.036865234
....略
```
#### RecognizeTextRequest


Recognizing the text content in images. (a.k.a. image to text)


![[Travelogue] 2023 Tokyo 5-Day Free Trip](../9da2c51fa4f2/)](/assets/755509180ca8/1*XL40lLT774PfO60rCIfnxA.jpeg)

[[Travelogue] 2023 Tokyo 5-Day Free Trip](../9da2c51fa4f2/)
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    var request = RecognizeTextRequest()
    request.recognitionLevel = .accurate
    request.recognitionLanguages = [.init(identifier: "ja-JP"), .init(identifier: "en-US")] // Specify language code, e.g., Traditional Chinese
    Task {
        do {
            let observations = try await request.perform(on: URL(string: "https://zhgchg.li/assets/9da2c51fa4f2/1*fBbNbDepYioQ-3-0XUkF6Q.jpeg")!)
            observations.forEach {
                observation in
                let topCandidate = observation.topCandidates(1).first
                print(topCandidate?.string ?? "No text recognized")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old syntax
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedTextObservation] else {
            return
        }
        observations.forEach {
            observation in
            let topCandidate = observation.topCandidates(1).first
            print(topCandidate?.string ?? "No text recognized")
        }
    }

    let request = VNRecognizeTextRequest(completionHandler: completionHandler)
    request.recognitionLevel = .accurate
    request.recognitionLanguages = ["ja-JP", "en-US"] // Specify language code, e.g., Traditional Chinese
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: URL(string: "https://zhgchg.li/assets/9da2c51fa4f2/1*fBbNbDepYioQ-3-0XUkF6Q.jpeg")!, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Result:**
```makefile
LE LABO Aoyama Store
TEL:03-6419-7167
＊Thank you for your purchase*
No: 21347
Date: 2023/06/10 14.14.57
Person in charge:
1690370
Register: 008A 1
Product Name
Tax-included Price Quantity Tax-included Total
Kaiak 10 EDP FB 15ML
J1P7010000S
16,800
16,800
Another 13 EDP FB 15ML
J1PJ010000S
10,700
10,700
Lip Balm 15ML
JOWC010000S
2,000
1
Total Amount
(Tax Amount Included)
CARD
2,000
3 items purchased
29,500
0
29,500
29,500
```
#### DetectBarcodesRequest

Detect barcode and QR code data in the image.


![](/assets/755509180ca8/1*Z72y9rIwIKQCmnnuwsq0uQ.png)



![Recommended Cooling Balm by Locals in Thailand](/assets/755509180ca8/1*s3V1UQRIqto-iG1e30PK7Q.jpeg)

Recommended Cooling Balm by Locals in Thailand
```swift
let filePath = Bundle.main.path(forResource: "IMG_6777", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = DetectBarcodesRequest()
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            observations.forEach {
                observation in
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old method
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        observations.forEach {
            observation in
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let request = VNDetectBarcodesRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Analysis Result:**
```makefile
Payload: 8859126000911
Symbology: VNBarcodeSymbologyEAN13
Payload: https://lin.ee/hGynbVM
Symbology: VNBarcodeSymbologyQR
Payload: http://www.hongthaipanich.com/
Symbology: VNBarcodeSymbologyQR
Payload: https://www.facebook.com/qr?id=100063856061714
Symbology: VNBarcodeSymbologyQR
```
#### RecognizeAnimalsRequest


Identify the animals in the image and their confidence levels.


![](/assets/755509180ca8/1*5zF3gA3WB1Q0-_cgt6mTCw.png)



![[meme Source](https://www.redbubble.com/i/canvas-print/Funny-AI-Woman-yelling-at-a-cat-meme-design-Machine-learning-by-omolog/43039298.5Y5V7){:target="_blank"}](/assets/755509180ca8/1*KZ7mdE8fobP-_oj7tJf_Ww.jpeg)

[meme Source](https://www.redbubble.com/i/canvas-print/Funny-AI-Woman-yelling-at-a-cat-meme-design-Machine-learning-by-omolog/43039298.5Y5V7){:target="_blank"}
```swift
let filePath = Bundle.main.path(forResource: "IMG_5026", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    let request = RecognizeAnimalsRequest()
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            observations.forEach {
                observation in
                let labels = observation.labels
                labels.forEach {
                    label in
                    print("Detected animal: \(label.identifier) with confidence: \(label.confidence)")
                }
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old method
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedObjectObservation] else {
            return
        }
        observations.forEach {
            observation in
            let labels = observation.labels
            labels.forEach {
                label in
                print("Detected animal: \(label.identifier) with confidence: \(label.confidence)")
            }
        }
    }

    let request = VNRecognizeAnimalsRequest(completionHandler: completionHandler)
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```


Analysis Result:
```csharp
Detected animal: Cat with confidence: 0.77245045
```
#### Others:
- Detect human bodies in images: DetectHumanRectanglesRequest
- Detect poses of humans and animals (both 3D and 2D are acceptable): DetectAnimalBodyPoseRequest, DetectHumanBodyPose3DRequest, DetectHumanBodyPoseRequest, DetectHumanHandPoseRequest
- Detect and track the motion trajectories of objects (in different detections in videos and animations): DetectTrajectoriesRequest, TrackObjectRequest, TrackRectangleRequest

#### **iOS ≥ 18 Update Highlight:**
```rust
VN*Request -> *Request (e.g. VNDetectBarcodesRequest -> DetectBarcodesRequest)
VN*Observation -> *Observation (e.g. VNRecognizedObjectObservation -> RecognizedObjectObservation)
VNRequestCompletionHandler -> async/await
VNImageRequestHandler.perform([VN*Request]) -> *Request.perform()
```
### WWDC Example

The official WWDC video uses a supermarket product scanner as an example.
#### First, most products have barcodes available for scanning


![](/assets/755509180ca8/1*YT_Uf8eEi36Iv7zcOrmP4A.png)



![](/assets/755509180ca8/1*J9uIwRKubLoJoC7i096AdQ.png)



![](/assets/755509180ca8/1*gKg-NfHYqy7uBqe5hxzBSw.png)


We can obtain the location of the barcode from `observation.boundingBox`, but unlike the common UIView coordinate system, the starting point of the `BoundingBox`'s relative position is from the bottom left corner, with values ranging from 0 to 1.
```swift
let filePath = Bundle.main.path(forResource: "IMG_6785", ofType: "png")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    var request = DetectBarcodesRequest()
    request.symbologies = [.ean13] // If only scanning EAN13 barcodes, can specify directly to improve performance
    Task {
        do {
            let observations = try await request.perform(on: fileURL)
            if let observation = observations.first {
                DispatchQueue.main.async {
                    self.infoLabel.text = observation.payloadString
                    // Color layer for marking
                    let colorLayer = CALayer()
                    // iOS >=18 new coordinate conversion API toImageCoordinates
                    // Not tested, may actually require calculating displacement for ContentMode = AspectFit:
                    colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                    colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                    self.baseImageView.layer.addSublayer(colorLayer)
                }
                print("BoundingBox: \(observation.boundingBox.cgRect)")
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old method
    let completionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        if let observation = observations.first {
            DispatchQueue.main.async {
                self.infoLabel.text = observation.payloadStringValue
                // Color layer for marking
                let colorLayer = CALayer()
                colorLayer.frame = self.convertBoundingBox(observation.boundingBox, to: self.baseImageView)
                colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                self.baseImageView.layer.addSublayer(colorLayer)
            }
            print("BoundingBox: \(observation.boundingBox)")
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let request = VNDetectBarcodesRequest(completionHandler: completionHandler)
    request.symbologies = [.ean13] // If only scanning EAN13 barcodes, can specify directly to improve performance
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([request])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

```markdown
**iOS ≥ 18 Update Highlight:**
```less
// iOS >=18 new coordinate conversion API toImageCoordinates
observation.boundingBox.toImageCoordinates(CGSize, origin: .upperLeft)
// https://developer.apple.com/documentation/vision/normalizedpoint/toimagecoordinates(from:imagesize:origin:)
```

**Helper:**
```swift
// Gen by ChatGPT 4o
// Because the photo in ImageView is set with ContentMode = AspectFit
// Therefore, we need to calculate the vertical displacement caused by Fit
func convertBoundingBox(_ boundingBox: CGRect, to view: UIImageView) -> CGRect {
    guard let image = view.image else {
        return .zero
    }

    let imageSize = image.size
    let viewSize = view.bounds.size
    let imageRatio = imageSize.width / imageSize.height
    let viewRatio = viewSize.width / viewSize.height
    var scaleFactor: CGFloat
    var offsetX: CGFloat = 0
    var offsetY: CGFloat = 0
    if imageRatio > viewRatio {
        // The image fits in width
        scaleFactor = viewSize.width / imageSize.width
        offsetY = (viewSize.height - imageSize.height * scaleFactor) / 2
    }

    else {
        // The image fits in height
        scaleFactor = viewSize.height / imageSize.height
        offsetX = (viewSize.width - imageSize.width * scaleFactor) / 2
    }

    let x = boundingBox.minX * imageSize.width * scaleFactor + offsetX
    let y = (1 - boundingBox.maxY) * imageSize.height * scaleFactor + offsetY
    let width = boundingBox.width * imageSize.width * scaleFactor
    let height = boundingBox.height * imageSize.height * scaleFactor
    return CGRect(x: x, y: y, width: width, height: height)
}
```

**Output Result**
```makefile
BoundingBox: (0.5295758928571429, 0.21408638121589782, 0.0943080357142857, 0.21254415360708087)
Payload: 4710018183805
Symbology: VNBarcodeSymbologyEAN13
```
#### Some products do not have a barcode, such as bulk fruits that only have product labels.


![](/assets/755509180ca8/1*jeZhLtg9j11kgOAvKZmevg.jpeg)



![](/assets/755509180ca8/1*YNokMMUewMA2kzjoGmMJPw.png)


Therefore, our scanner also needs to support scanning plain text labels.
```swift
let filePath = Bundle.main.path(forResource: "apple", ofType: "jpg")! // Local test image
let fileURL = URL(filePath: filePath)
if #available(iOS 18.0, *) {
    // New API using Swift features
    var barcodesRequest = DetectBarcodesRequest()
    barcodesRequest.symbologies = [.ean13] // If only scanning EAN13 Barcode, can specify directly to improve performance
    var textRequest = RecognizeTextRequest()
    textRequest.recognitionLanguages = [.init(identifier: "zh-Hnat"), .init(identifier: "en-US")]
    Task {
        do {
            let handler = ImageRequestHandler(fileURL)
            // parameter pack syntax and we must wait for all requests to finish before we can use their results.
            // let (barcodesObservation, textObservation, ...) = try await handler.perform(barcodesRequest, textRequest, ...)
            let (barcodesObservation, textObservation) = try await handler.perform(barcodesRequest, textRequest)
            if let observation = barcodesObservation.first {
                DispatchQueue.main.async {
                    self.infoLabel.text = observation.payloadString
                    // Color layer for marking
                    let colorLayer = CALayer()
                    // iOS >=18 new coordinate conversion API toImageCoordinates
                    // Not tested, actual calculations may still need to account for the displacement of ContentMode = AspectFit:
                    colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                    colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                    self.baseImageView.layer.addSublayer(colorLayer)
                }
                print("BoundingBox: \(observation.boundingBox.cgRect)")
                print("Payload: \(observation.payloadString ?? "No payload")")
                print("Symbology: \(observation.symbology)")
            }
            textObservation.forEach {
                observation in
                let topCandidate = observation.topCandidates(1).first
                print(topCandidate?.string ?? "No text recognized")
            }
        }
        catch {
            print("Request failed: \(error)")
        }
    }
} else {
    // Old method
    let barcodesCompletionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNBarcodeObservation] else {
            return
        }
        if let observation = observations.first {
            DispatchQueue.main.async {
                self.infoLabel.text = observation.payloadStringValue
                // Color layer for marking
                let colorLayer = CALayer()
                colorLayer.frame = self.convertBoundingBox(observation.boundingBox, to: self.baseImageView)
                colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                self.baseImageView.layer.addSublayer(colorLayer)
            }
            print("BoundingBox: \(observation.boundingBox)")
            print("Payload: \(observation.payloadStringValue ?? "No payload")")
            print("Symbology: \(observation.symbology.rawValue)")
        }
    }

    let textCompletionHandler: VNRequestCompletionHandler = {
        request, error in
        guard error == nil else {
            print("Request failed: \(String(describing: error))")
            return
        }
        guard let observations = request.results as? [VNRecognizedTextObservation] else {
            return
        }
        observations.forEach {
            observation in
            let topCandidate = observation.topCandidates(1).first
            print(topCandidate?.string ?? "No text recognized")
        }
    }

    let barcodesRequest = VNDetectBarcodesRequest(completionHandler: barcodesCompletionHandler)
    barcodesRequest.symbologies = [.ean13] // If only scanning EAN13 Barcode, can specify directly to improve performance
    let textRequest = VNRecognizeTextRequest(completionHandler: textCompletionHandler)
    textRequest.recognitionLevel = .accurate
    textRequest.recognitionLanguages = ["en-US"]
    DispatchQueue.global().async {
        let handler = VNImageRequestHandler(url: fileURL, options: [:])
        do {
            try handler.perform([barcodesRequest, textRequest])
        }
        catch {
            print("Request failed: \(error)")
        }
    }
}
```

**Output Result:**
```
94128s
ORGANIC
Pink Lady®
Produce of USh
```

**iOS ≥ 18 Update Highlight:**
```swift
let handler = ImageRequestHandler(fileURL)
// parameter pack syntax and we must wait for all requests to finish before we can use their results.
// let (barcodesObservation, textObservation, ...) = try await handler.perform(barcodesRequest, textRequest, ...)
let (barcodesObservation, textObservation) = try await handler.perform(barcodesRequest, textRequest)
```
#### iOS ≥ 18 [performAll\( \)](https://developer.apple.com/documentation/vision/imagerequesthandler/performall(_:)?changes=latest_minor){:target="_blank"} Method


![](/assets/755509180ca8/1*z0364eYD4F4On194EgQ1kQ.png)


The previous `perform(barcodesRequest, textRequest)` method for handling barcode scanning and text scanning requires waiting for both requests to complete before proceeding; starting from iOS 18, a new `performAll()` method is provided, changing the response method to streaming, allowing for immediate handling upon receiving the result of one of the requests, such as responding directly when a barcode is scanned.
```swift
if #available(iOS 18.0, *) {
    // New API using Swift features
    var barcodesRequest = DetectBarcodesRequest()
    barcodesRequest.symbologies = [.ean13] // If only EAN13 Barcode scanning is needed, it can be specified directly to improve performance
    var textRequest = RecognizeTextRequest()
    textRequest.recognitionLanguages = [.init(identifier: "zh-Hnat"), .init(identifier: "en-US")]
    Task {
        let handler = ImageRequestHandler(fileURL)
        let observation = handler.performAll([barcodesRequest, textRequest] as [any VisionRequest])
        for try await result in observation {
            switch result {
                case .detectBarcodes(_, let barcodesObservation):
                if let observation = barcodesObservation.first {
                    DispatchQueue.main.async {
                        self.infoLabel.text = observation.payloadString
                        // Mark color Layer
                        let colorLayer = CALayer()
                        // iOS >=18 new coordinate transformation API toImageCoordinates
                        // Not tested, actual calculations may still need to account for ContentMode = AspectFit displacement:
                        colorLayer.frame = observation.boundingBox.toImageCoordinates(self.baseImageView.frame.size, origin: .upperLeft)
                        colorLayer.backgroundColor = UIColor.red.withAlphaComponent(0.5).cgColor
                        self.baseImageView.layer.addSublayer(colorLayer)
                    }
                    print("BoundingBox: \(observation.boundingBox.cgRect)")
                    print("Payload: \(observation.payloadString ?? "No payload")")
                    print("Symbology: \(observation.symbology)")
                }
                case .recognizeText(_, let textObservation):
                textObservation.forEach {
                    observation in
                    let topCandidate = observation.topCandidates(1).first
                    print(topCandidate?.string ?? "No text recognized")
                }
                default:
                print("Unrecognized result: \(result)")
            }
        }
    }
}
```
### Optimize with Swift Concurrency

![](/assets/755509180ca8/1*LgxxMOVS6is3n6EqPWqA6Q.png)

![](/assets/755509180ca8/1*80CFJpkb-gjy3bJs4jAC2A.png)

Assuming we have a wall of images, where each image needs to be automatically cropped to focus on the main subject; we can effectively utilize Swift Concurrency to enhance loading efficiency.
#### **Original Implementation**
```swift
func generateThumbnail(url: URL) async throws -> UIImage {
  let request = GenerateAttentionBasedSaliencyImageRequest()
  let saliencyObservation = try await request.perform(on: url)
  return cropImage(url, to: saliencyObservation.salientObjects)
}
    
func generateAllThumbnails() async throws {
  for image in images {
    image.thumbnail = try await generateThumbnail(url: image.url)
  }
}
```

Only one task is executed at a time, resulting in slow efficiency and performance.
#### **Optimization (1) — TaskGroup** Concurrency
```swift
func generateAllThumbnails() async throws {
  try await withThrowingDiscardingTaskGroup { taskGroup in
    for image in images {
      image.thumbnail = try await generateThumbnail(url: image.url)
     }
  }
}
```

Each task is added to the TaskGroup for concurrent execution.

> **_Issue: Image recognition and screenshot operations are very memory-intensive. If parallel tasks are added indiscriminately, it may cause user lag and OOM crash issues._**

#### Optimization (2) — TaskGroup Concurrency + Limit Concurrent Tasks
```swift
func generateAllThumbnails() async throws {
    try await withThrowingDiscardingTaskGroup {
        taskGroup in
        // Maximum number of concurrent tasks should not exceed 5
        let maxImageTasks = min(5, images.count)
        // Initially fill 5 tasks
        for index in 0..<maxImageTasks {
            taskGroup.addTask {
                image[index].thumbnail = try await generateThumbnail(url: image[index].url)
            }
        }
        var nextIndex = maxImageTasks
        for try await _ in taskGroup {
            // When a task in taskGroup completes...
            // Check if the index has reached the end
            if nextIndex < images.count {
                let image = images[nextIndex]
                // Continue to fill tasks one by one (maintaining a maximum of 5)
                taskGroup.addTask {
                    image.thumbnail = try await generateThumbnail(url: image.url)
                }
                nextIndex += 1
            }
        }
    }
}
```
### Update an existing Vision app

![](/assets/755509180ca8/1*0OhzcxQ7OpSujeyvt9918Q.png)

![](/assets/755509180ca8/1*MH4Xa0RB2DZQ1Fl9-kItSw.png)

```markdown
1. Vision will remove CPU and GPU support for certain requests on devices equipped with a neural engine. On these devices, the neural engine is the best choice for performance.
You can check using the `supportedComputeDevices()` API.
2. Remove all VN prefixes.
`VNXXRequest`, `VNXXXObservation` -> `Request`, `Observation`
3. Replace the original VNRequestCompletionHandler with async/await.
4. Directly use `*Request.perform()` instead of the original `VNImageRequestHandler.perform([VN*Request])`.

### Wrap-up
- APIs designed for Swift language features.
- New functionalities and methods are Swift Only, available on iOS ≥ 18.
- New image scoring feature, body + hand motion tracking.

### Thanks\!

![](/assets/755509180ca8/1*BK_5eH1i4-drOUOGnuQRSg.png)

### KKday Recruitment

![](/assets/755509180ca8/1*kjcldhvCP1cM-QqDfRFaYg.png)

👉👉👉This reading session is based on the weekly technical sharing activities within the KKday App Team. **The team is currently enthusiastically recruiting [Senior iOS Engineer](https://kkday.bamboohr.com/careers/25?source=aWQ9Mjk%3D){:target="_blank"}, interested friends are welcome to submit their resumes.** 👈👈👈
#### Reference Material
#### [Discover Swift enhancements in the Vision framework](https://developer.apple.com/videos/play/wwdc2024/10163/){:target="_blank"}

The Vision Framework API has been redesigned to leverage modern Swift features like concurrency, making it easier and faster to integrate a wide array of Vision algorithms into your app. We’ll tour the updated API and share sample code, along with best practices, to help you get the benefits of this framework with less coding effort. We’ll also demonstrate two new features: image aesthetics and holistic body pose.
### Chapters
- 0:00 — [Introduction](https://developer.apple.com/videos/play/wwdc2024/10163/?time=0){:target="_blank"}
- 1:07 — [New Vision API](https://developer.apple.com/videos/play/wwdc2024/10163/?time=67){:target="_blank"}
- 1:47 — [Get started with Vision](https://developer.apple.com/videos/play/wwdc2024/10163/?time=107){:target="_blank"}
- 8:59 — [Optimize with Swift Concurrency](https://developer.apple.com/videos/play/wwdc2024/10163/?time=539){:target="_blank"}
- 11:05 — [Update an existing Vision app](https://developer.apple.com/videos/play/wwdc2024/10163/?time=665){:target="_blank"}
- 13:46 — [What’s new in Vision?](https://developer.apple.com/videos/play/wwdc2024/10163/?time=826){:target="_blank"}

#### [Vision framework Apple Developer Documentation](https://developer.apple.com/documentation/vision/){:target="_blank"}
```

- 

If you have any questions or suggestions, feel free to [contact me](https://www.zhgchg.li/contact){:target="_blank"}.

_[Post](https://medium.com/kkdaytech/ios-vision-framework-x-wwdc-24-discover-swift-enhancements-in-the-vision-framework-session-755509180ca8){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
