---
title: "Vision Exploration — Automatic Face Cropping for APP Avatar Upload (Swift)"
author: "ZhgChgLi"
date: 2018-10-16T16:01:24.511+0000
last_modified_at: 2024-08-13T08:17:24.185+0000
categories: ["ZRealm Dev."]
tags: ["swift","machine-learning","facedetection","ios","ios-app-development"]
description: "Practical Applications of Vision"
image:
  path: /assets/9a9aa892f9a9/1*c-ioRH_Z2nMYRxSbuBD71A.png
render_with_liquid: false
---

### Vision Exploration — Automatic Face Cropping for APP Avatar Upload \(Swift\)

Practical Applications of Vision

### \[2024/08/13 Update\]
- Please refer to the new article and new API: 「 [iOS Vision framework x WWDC 24 Discover Swift enhancements in the Vision framework Session](../755509180ca8/) 」

#### Without further ado, here’s a finished product image:


![Before Optimization V\.S After Optimization — [Marriage APP](https://itunes.apple.com/tw/app/%E7%B5%90%E5%A9%9A%E5%90%A7-%E4%B8%8D%E6%89%BE%E6%9C%80%E8%B2%B4-%E5%8F%AA%E6%89%BE%E6%9C%80%E5%B0%8D/id1356057329?ls=1&mt=8){:target="_blank"}](/assets/9a9aa892f9a9/1*c-ioRH_Z2nMYRxSbuBD71A.png)

Before Optimization V\.S After Optimization — [Marriage APP](https://itunes.apple.com/tw/app/%E7%B5%90%E5%A9%9A%E5%90%A7-%E4%B8%8D%E6%89%BE%E6%9C%80%E8%B2%B4-%E5%8F%AA%E6%89%BE%E6%9C%80%E5%B0%8D/id1356057329?ls=1&mt=8){:target="_blank"}

Recently, with the release of iOS 12, I noticed the newly opened CoreML machine learning framework; I found it quite interesting and began to think about how it could be applied to current products.


> **CoreML tasting article has now been published: [Using machine learning to automatically predict article classification, even training the model itself](../793bf2cdda0f/)** 

CoreML provides interfaces for training and referencing machine learning models for text and images in apps. My initial idea was to use CoreML for face recognition to solve the problem of cropping where the head or face gets cut off in the app, as shown in the left image above. If a face appears at the edges, it can easily become incomplete due to scaling and cropping.

After some online research, I realized my knowledge was limited; this feature was already released in iOS 11: the "Vision" framework, which supports text detection, face detection, image matching, QR code detection, object tracking, and more.

What I’m using here is the face detection feature, which, after optimization, is shown in the right image; it finds the face and crops the image centered around it.
### Practical Implementation:
#### First, let's create a feature that can mark the position of faces, getting a preliminary understanding of how to use Vision.


![Demo APP](/assets/9a9aa892f9a9/1*cpGgpXsBhuiJoZI03WAGUw.png)

Demo APP

The completed image is shown above, marking the position of faces in the photo.

p\.s It can only mark "faces"; the entire head including hair cannot be marked 😅

This section of the code is mainly divided into two parts. The first part addresses the issue of leaving white space when scaling the original image size into the ImageView. In simple terms, we want the Image size to match the ImageView size. If we directly place the image, it can cause the following misalignment.


![](/assets/9a9aa892f9a9/1*Mb70Ed6pALO-8sllCpb7Qg.png)


You might think of directly changing the ContentMode to fill, fit, or redraw, but that would distort the image or cut it off.
```swift
let ratio = UIScreen.main.bounds.size.width
// This is because I set the left and right alignment of the UIImageView to 0, with an aspect ratio of 1:1.

let sourceImage = UIImage(named: "Demo2")?.kf.resize(to: CGSize(width: ratio, height: CGFloat.leastNonzeroMagnitude), for: .aspectFill)
// Using KingFisher's image resizing feature, with width as the basis and height flexible.

imageView.contentMode = .redraw
// Using redraw for contentMode to fill.

imageView.image = sourceImage
// Assigning the image.

imageViewConstraints.constant = (ratio - (sourceImage?.size.height ?? 0))
imageView.layoutIfNeeded()
imageView.sizeToFit()
// This part is where I change the imageView's constraints; for details, see the complete example at the end.
```


The above is the processing done for the image.

_Cropping is assisted by Kingfisher, but it can be replaced with other libraries or custom methods._

In the second part, let's get straight to the code.
```swift
if #available(iOS 11.0, *) {
    // Supported only after iOS 11
    let completionHandle: VNRequestCompletionHandler = { request, error in
        if let faceObservations = request.results as? [VNFaceObservation] {
            // Detected faces
            
            DispatchQueue.main.async {
                // Manipulate UIView, switch back to the main thread
                let size = self.imageView.frame.size
                
                faceObservations.forEach({ (faceObservation) in
                    // Coordinate system transformation
                    let translate = CGAffineTransform.identity.scaledBy(x: size.width, y: size.height)
                    let transform = CGAffineTransform(scaleX: 1, y: -1).translatedBy(x: 0, y: -size.height)
                    let transRect =  faceObservation.boundingBox.applying(translate).applying(transform)
                    
                    let markerView = UIView(frame: transRect)
                    markerView.backgroundColor = UIColor.init(red: 0/255, green: 255/255, blue: 0/255, alpha: 0.3)
                    self.imageView.addSubview(markerView)
                })
            }
        } else {
            print("No faces detected")
        }
    }
    
    // Detection request
    let baseRequest = VNDetectFaceRectanglesRequest(completionHandler: completionHandle)
    let faceHandle = VNImageRequestHandler(ciImage: ciImage, options: [:])
    DispatchQueue.global().async {
        // Detection takes time, so run it in a background thread to avoid freezing the current screen
        do{
            try faceHandle.perform([baseRequest])
        }catch{
            print("Throws：\(error)")
        }
    }
  
} else {
    //
    print("Not supported")
}
```

The main thing to note is the coordinate system transformation; the results detected are in the original coordinates of the image; we need to convert them to the actual coordinates of the enclosing ImageView to use them correctly.
#### Next, let's do the main event of today — cropping the correct position for the profile picture based on the position of the face.
```php
let ratio = UIScreen.main.bounds.size.width
// This is because I set the UIImageView to align left and right to 0, with an aspect ratio of 1:1. For details, see the complete example at the end of the article.

let sourceImage = UIImage(named: "Demo")

imageView.contentMode = .scaleAspectFill
// Use scaleAspectFill mode to fill

imageView.image = sourceImage
// Directly assign the original image, we will manipulate it later

if let image = sourceImage,#available(iOS 11.0, *),let ciImage = CIImage(image: image) {
    let completionHandle: VNRequestCompletionHandler = { request, error in
        if request.results?.count == 1,let faceObservation = request.results?.first as? VNFaceObservation {
            // One face
            let size = CGSize(width: ratio, height: ratio)
            
            let translate = CGAffineTransform.identity.scaledBy(x: size.width, y: size.height)
            let transform = CGAffineTransform(scaleX: 1, y: -1).translatedBy(x: 0, y: -size.height)
            let finalRect =  faceObservation.boundingBox.applying(translate).applying(transform)
            
            let center = CGPoint(x: (finalRect.origin.x + finalRect.width/2 - size.width/2), y: (finalRect.origin.y + finalRect.height/2 - size.height/2))
            // Here we calculate the midpoint of the face's bounding area
            
            let newImage = image.kf.resize(to: size, for: .aspectFill).kf.crop(to: size, anchorOn: center)
            // Crop the image based on the center point
            
            DispatchQueue.main.async {
                // Manipulate UIView, switch back to the main thread
                self.imageView.image = newImage
            }
        } else {
            print("Detected multiple faces or no faces detected")
        }
    }
    let baseRequest = VNDetectFaceRectanglesRequest(completionHandler: completionHandle)
    let faceHandle = VNImageRequestHandler(ciImage: ciImage, options: [:])
    DispatchQueue.global().async {
        do{
            try faceHandle.perform([baseRequest])
        }catch{
            print("Throws：\(error)")
        }
    }
} else {
    print("Not supported")
}
```

The principle is similar to marking the position of a face, with the difference being that the profile picture has a fixed size (e.g., 300x300), so we skip the initial part where the Image needs to fit the ImageView.

Another difference is that we need to calculate the center point of the face area and use this center point as the basis for cropping the image.

![The red dot indicates the center point of the face area](/assets/9a9aa892f9a9/1*civytcKOguHfVFHYPVWecA.png)

The red dot indicates the center point of the face area.
#### Completed Effect Image:

![The original image position is the second before the freeze frame](/assets/9a9aa892f9a9/1*WocYjt0xLkqtGVilxfT2LA.gif)

The original image position is the second before the freeze frame.
### Complete APP Example:

![](/assets/9a9aa892f9a9/1*J8oByw8gBCamIac2TkT1SA.gif)

The code has been uploaded to GitHub: [Click here](https://github.com/zhgchgli0718/VisionDemo){:target="_blank"}

If you have any questions or suggestions, feel free to [contact me](https://www.zhgchg.li/contact){:target="_blank"}.

_[Post](https://medium.com/zrealm-ios-dev/vision-%E5%88%9D%E6%8E%A2-app-%E9%A0%AD%E5%83%8F%E4%B8%8A%E5%82%B3-%E8%87%AA%E5%8B%95%E8%AD%98%E5%88%A5%E4%BA%BA%E8%87%89%E8%A3%81%E5%9C%96-swift-9a9aa892f9a9){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
